{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from time import time\n",
    "from os.path import join as join_path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import logging # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from utils import clean_text\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "cord_data_dir = 'data'\n",
    "cord_data_path = join_path(cord_data_dir, 'cord-19-data.csv')\n",
    "d2v_saved_models_dir = 'models-doc2vec'\n",
    "saved_models_prefix = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord_data = pd.read_csv(cord_data_path)\n",
    "cord_data_eng = cord_data[cord_data['language'] == 'en']\n",
    "eng_texts = cord_data_eng[['cord_uid', 'body_text']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cord_num_sentences = 0\n",
    "for _, text in tqdm(eng_texts):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    cord_num_sentences += len(sentences)\n",
    "print(f'Total number of CORD-19 sentences: {cord_num_sentences}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CORDDataIteratorDoc2Vec():\n",
    "    def __init__(self, texts: np.ndarray):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for cord_uid, cord_text in self.texts:\n",
    "            sentences = nltk.tokenize.sent_tokenize(cord_text)\n",
    "            cleaned_sentences = [clean_text(sent) for sent in sentences]\n",
    "            for sentence in cleaned_sentences:\n",
    "                yield TaggedDocument(sentence, [cord_uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord_sentences = CORDDataIteratorDoc2Vec(eng_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn word embeddings using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocEpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self, output_dir: str, prefix: str, start_epoch: int = 1):\n",
    "        self.output_dir = output_dir\n",
    "        self.prefix = prefix\n",
    "        self.epoch = start_epoch\n",
    "\n",
    "    def on_epoch_end(self, model):        \n",
    "        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch + 1}.model')\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup initial model\n",
    "d2v_model = Doc2Vec(\n",
    "    min_count=20,\n",
    "    window=2,\n",
    "    vector_size=300,\n",
    "    negative=5,\n",
    "    workers=cores-1,\n",
    "    callbacks=[DocEpochSaver(d2v_saved_models_dir, saved_models_prefix)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "t = time()\n",
    "d2v_model.build_vocab(tqdm(cord_sentences, total=cord_num_sentences), progress_per=int(cord_num_sentences / 100))\n",
    "print(f'Time to build vocab: {round((time() - t) / 60, 2)} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2v_model = Word2Vec.load('models-doc2vec/model_epoch_2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "t = time()\n",
    "d2v_model.train(cord_sentences, total_examples=d2v_model.corpus_count, epochs=10, report_delay=30, callbacks=[DocEpochSaver(d2v_saved_models_dir, saved_models_prefix, 10)])\n",
    "print(f'Time to train the model: {round((time() - t) / 60, 2)} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2v_model.save('models-doc2vec/model_epoch_2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype search pipeline below\n",
    "'''\n",
    "len(d2v_model.docvecs.index2entity)\n",
    "\n",
    "query = clean_text('The patient (Fo, ) was a 58 year old mentally retarded white woman, born in a rural area of southwestern Virginia.')\n",
    "query_vec = d2v_model.infer_vector(query, epochs=100)\n",
    "\n",
    "eng_texts[0][0]\n",
    "\n",
    "doc_weight_mat = np.zeros((len(d2v_model.docvecs.index2entity), 300))\n",
    "for i, cord_uid in enumerate(tqdm(d2v_model.docvecs.index2entity)):\n",
    "    doc_weight_mat[i] = d2v_model.docvecs[cord_uid]\n",
    "\n",
    "def cosine_sim(vec: np.ndarray, mat: np.ndarray):\n",
    "    return vec @ mat.T / (np.linalg.norm(vec) * np.linalg.norm(mat, axis=1))\n",
    "\n",
    "query_vec.shape, doc_weight_mat.shape\n",
    "\n",
    "# Find closest document\n",
    "#keys = d2v_model.docvecs.index2entity\n",
    "similarities = cosine_sim(query_vec, doc_weight_mat)\n",
    "\n",
    "top_n = 10\n",
    "sorted_indicies = similarities.argsort()[::-1]\n",
    "top_sim = list(zip(np.array(d2v_model.docvecs.index2entity)[sorted_indicies][:top_n], similarities[sorted_indicies][:top_n]))\n",
    "top_sim\n",
    "\n",
    "top_cord_uid = top_sim[0][0]\n",
    "best_text = cord_data[cord_data['cord_uid'] == top_cord_uid.split('_')[0]].body_text.values[0]\n",
    "best_text_sentences = nltk.tokenize.sent_tokenize(best_text)\n",
    "best_text_sentences[int(top_cord_uid.split('_')[1])]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
