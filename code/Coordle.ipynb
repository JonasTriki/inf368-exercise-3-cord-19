{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/jonavin/anaconda3/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import nltk\n",
    "import spacy\n",
    "from coordle.utils import clean_text\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from collections.abc import Iterable\n",
    "from typing import Union\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "import pymongo\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re \n",
    "from string import punctuation as PUNCTUATION\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from os.path import join as join_path\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cord-19-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "class DocEpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last trained model\n",
    "model = Word2Vec.load(join_path('data', 'cord-19-w2v.model'))\n",
    "word_to_int = {word:i for i, word in enumerate(model.wv.index2word)}\n",
    "int_to_word = np.array(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning initilized on 16 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning texts: 100%|██████████| 16/16 [00:00<00:00, 115.54it/s]\n",
      "Adding to index: 100%|██████████| 16/16 [00:00<00:00, 367.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import coordle\n",
    "reload(coordle)\n",
    "reload(coordle.backend)\n",
    "reload(coordle.utils)\n",
    "reload(coordle.preprocessing)\n",
    "from coordle.backend import (CordDoc, Index, RecursiveDescentParser, \n",
    "                             QueryAppenderIndex)\n",
    "\n",
    "\n",
    "ai_index = QueryAppenderIndex(model.wv.most_similar, n_similars=1)\n",
    "ai_index.build_from_df(\n",
    "    df[:16],\n",
    "    'cord_uid',\n",
    "    'title',\n",
    "    'body_text', \n",
    "    verbose=True, \n",
    "    use_multiprocessing=True,\n",
    "    workers=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque(['virus', 'OR', 'viral', 'NOT', 'white', 'OR', 'cat', 'OR', 'dog'])\n",
      "1wswi7us  Relationship of SARS-CoV to other pathogenic RNA viruses explored by t  36.8499\n",
      "qj4dh6rg  Cloaked similarity between HIV-1 and SARS-CoV suggests an anti-SARS st  18.4618\n",
      "xqhn0vbp  Airborne rhinovirus detection and effect of ultraviolet irradiation on  14.1420\n",
      "ng4rrdte  Pro/con clinical debate: Steroids are a key component in the treatment  8.9755\n",
      "5s6acr7m  The Virus That Changed My World                                         8.0834\n",
      "gi6uaa83  Discovering human history from stomach bacteria                         7.3727\n",
      "0qaoam29  A double epidemic model for the SARS propagation                        3.9388\n",
      "1ul8owic  Air pollution and case fatality of SARS in the People's Republic of Ch  1.1574\n",
      "fy4w7xz8  Association of HLA class I with severe acute respiratory syndrome coro  0.6953\n",
      "kuybfc1y  Descriptive review of geographic mapping of severe acute respiratory s  0.3231\n"
     ]
    }
   ],
   "source": [
    "import coordle\n",
    "reload(coordle)\n",
    "from coordle.backend import CordDoc, Index, RecursiveDescentParser, QueryAppenderIndex\n",
    "\n",
    "docs, scores, errmsgs = ai_index.search('virus NOT white cat')\n",
    "\n",
    "n = 69\n",
    "if errmsgs:\n",
    "    print(errmsgs)\n",
    "else:\n",
    "    for doc, score in zip(docs, scores):\n",
    "        print(f'{doc.uid}  {str(doc.title)[:70]:<70}  {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_index.docmap['grace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(coordle)\n",
    "reload(coordle.backend)\n",
    "from coordle import backend\n",
    "\n",
    "# from coordle.backend import (CordDoc, Index, RecursiveDescentParser, QueryAppenderIndex)\n",
    "\n",
    "# ram_index = Index()\n",
    "ram_index = backend.QueryAppenderIndex(model.wv.most_similar, 1)\n",
    "ram_index.build_from_df(\n",
    "    df[:128],\n",
    "    'cord_uid',\n",
    "    'title',\n",
    "    'body_text', \n",
    "    verbose=True, \n",
    "    use_multiprocessing=True,\n",
    "    workers=-1\n",
    ")\n",
    "\n",
    "docs, scores, errmsgs = ram_index.search('virus NOT cat white car AND virus')\n",
    "\n",
    "if errmsgs:\n",
    "    print(errmsgs)\n",
    "else:\n",
    "    for doc, score in zip(docs, scores):\n",
    "        print(f'{doc.uid}   {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coordle_mongobackend as cm\n",
    "reload(cm)\n",
    "\n",
    "# mongoindex = cm.Index('coordle')\n",
    "mongoindex = cm.AI_Index('coordle-lite', model.wv.most_similar, 1)\n",
    "mongoindex.drop_old_collections()\n",
    "mongoindex.build_from_df(df[:128], 'cord_uid', 'title', 'body_text', \n",
    "                         use_multiprocessing=True, workers=-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs, scores, errmsgs = ram_index.search('virus NOT cat white car AND virus')\n",
    "\n",
    "if errmsgs:\n",
    "    print(errmsgs)\n",
    "else:\n",
    "    for doc, score in zip(docs, scores):\n",
    "        print(f'{doc.uid}   {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "docs, scores, errmsgs = mongoindex.search('virus')\n",
    "\n",
    "if errmsgs:\n",
    "    print(errmsgs)\n",
    "else:\n",
    "    for doc, score in zip(docs, scores):\n",
    "        print(f'{doc}   {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmap = {word:[doc.uid for doc in docs] for word, docs in ai_index.docmap.items() if len(word) < 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()\n",
    "db_test = client['coordle']\n",
    "db_test.wordcounts.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, counts = np.unique(['a','a','a','b','b','b','b','c'], return_counts=True)\n",
    "print(vals)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = pd.value_counts(['a','a','a','b','b','b','b','c'])\n",
    "print(thing.index.values)\n",
    "print(thing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<center><h2> Junkyard </h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating sets from dicts is faster than from lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "dict_time = []\n",
    "list_time = []\n",
    "ratios = []\n",
    "\n",
    "xrange = np.arange(1000,1000000,1000)\n",
    "B = set(list(range(100000)))\n",
    "\n",
    "for j in xrange:\n",
    "    a = list(range(j))\n",
    "\n",
    "    t0=time()\n",
    "    A = set(a)\n",
    "    A | B\n",
    "    t1_list = time()-t0\n",
    "    list_time.append(t1_list)\n",
    "    \n",
    "    a = {i:None for i in range(j)}\n",
    "    \n",
    "    t0=time()\n",
    "#     A = set(a)\n",
    "    a.keys() | B\n",
    "    t1_dict = time()-t0\n",
    "    dict_time.append(t1_dict)\n",
    "\n",
    "    ratios.append(t1_list/t1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xrange[1:], dict_time[1:], label='dict')\n",
    "plt.plot(xrange[1:], list_time[1:], label='list')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xrange[1:], ratios[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import CordDoc, Index2, RecursiveDescentParser, AI_Index2\n",
    "Index = Index2\n",
    "\n",
    "queries = [\n",
    "    'retarded!!!',\n",
    "#     '(',\n",
    "#     ')',\n",
    "    'retarded (white AND (woman NOT man))',\n",
    "    'retarded (white AND white) man',\n",
    "#     'retarded OR white OR woman',\n",
    "#     'retarded white AND woman',\n",
    "#     'retarded OR white NOT woman',\n",
    "#     'retarded (white NOT woman)',\n",
    "#     'retarded (white NOT woman)',\n",
    "#     'OR retarded AND white woman',\n",
    "#     'retarded AND AND white NOT woman',\n",
    "#     'retarded (white NOT woman) AND',\n",
    "#     ')retarded ((white NOT woman) AND',\n",
    "#     'retarded ((white NOT woman)',\n",
    "#     'AND retarded)) ((white NOT woman) NOT',\n",
    "]\n",
    "\n",
    "rdp = RecursiveDescentParser(index.docmap)\n",
    "for query in queries:\n",
    "    errmsgs = []\n",
    "    tokens = rdp.get_logical_querytokens(query)\n",
    "    pass_ = rdp.assert_query(tokens, errmsgs)\n",
    "    \n",
    "#     print(tokens)\n",
    "    print(rdp.parenthesis_handler(tokens))\n",
    "#     print(errmsgs)\n",
    "    print(pass_)\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
