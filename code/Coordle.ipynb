{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Idea\n",
    "1. Get query, e.g. \"What is love?\"\n",
    "2. Tokenize query\n",
    "3. Create a vector using word2vec of the tokens by summing them, or finding the average or whatever.\n",
    "   lets call it the query vector. \n",
    "4. Find sentence vectors that are the closesest to the query vector.\n",
    "5. Return the papers corresponding to said sentence vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Frontend\n",
    "ðŸ’©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import nltk\n",
    "import spacy\n",
    "import en_core_sci_lg # Biomedical word embeddings\n",
    "from utils import clean_text, load_pickle, save_pickle\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from collections.abc import Iterable\n",
    "from typing import Union\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re \n",
    "from string import punctuation as PUNCTUATION\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from os.path import join as join_path\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cord-19-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self, output_dir: str, prefix: str, logs_filename: str):\n",
    "        self.output_dir = output_dir\n",
    "        self.prefix = prefix\n",
    "        self.logs_filename = logs_filename\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        cum_loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            loss = cum_loss\n",
    "        else:\n",
    "            loss = cum_loss - self.loss_previous_step\n",
    "        self.loss_previous_step = loss\n",
    "        with open(join_path(self.output_dir, self.logs_filename), 'a+') as file:\n",
    "            file.write(f'Epoch #{self.epoch}, loss: {loss}\\n')\n",
    "        \n",
    "        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch}.model')\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1    \n",
    "\n",
    "class DocEpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self, output_dir: str, prefix: str, start_epoch: int = 1):\n",
    "        self.output_dir = output_dir\n",
    "        self.prefix = prefix\n",
    "        self.epoch = start_epoch\n",
    "\n",
    "    def on_epoch_end(self, model):        \n",
    "        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch}.model')\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last trained model\n",
    "model = Word2Vec.load(join_path('models-word2vec', 'w2v_model_epoch_29.model'))\n",
    "word_to_int = {word:i for i, word in enumerate(model.wv.index2word)}\n",
    "int_to_word = np.array(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning initilized on 16 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:03<00:00, 75.38it/s]\n",
      "Adding to index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:00<00:00, 454.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import SentVectorDoc, Index\n",
    "\n",
    "def test_SentVectorDoc():\n",
    "    docsample = df[df.cord_uid == 'vs5yondw']\n",
    "#     doc = SentVectorDoc(docsample['cord_uid'], docsample['title'])\n",
    "#     doc, _ = doc.fit(docsample['body_text'])\n",
    "    gaga = Index()\n",
    "    gaga.build_from_df(\n",
    "        docsample,\n",
    "#         df[:32],\n",
    "        'cord_uid',\n",
    "        'title',\n",
    "        'body_text', \n",
    "        verbose=True, \n",
    "        use_multiprocessing=False,\n",
    "        workers=-1\n",
    "    )\n",
    "    \n",
    "    return gaga\n",
    "#     return doc\n",
    "\n",
    "def test_Index():\n",
    "    coordle = Index()\n",
    "    coordle.build_from_df(\n",
    "        df.iloc[:256],\n",
    "        'cord_uid',\n",
    "        'title',\n",
    "        'body_text', \n",
    "        verbose=True, \n",
    "        use_multiprocessing=True,\n",
    "        workers=-1\n",
    "    )\n",
    "    return coordle\n",
    "\n",
    "index = test_Index()\n",
    "# fuck = test_SentVectorDoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle(index, 'coordle/coordle_index_8k_0904.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens:  ['retarded', 'white', 'woman']\n",
      "[8bnny6hj, zp9k1k3z, 5xswwney, fdtwagr1, hnl4a33n, 1rzbaj02, 15mv5yx7, 2ole6ya7, vs5yondw, j1iylpgm, ltf0xacf, eyk015n3, bgygebgx, mnhg329v, y2ruobm0, ak7bf0gq, uq88w4fm, i94lyfsh, lno01041, y2uhnlpd, ensep6lk, 9jpc42h1, tjzlssal, 34jn37sb, p8bzkmas, 67kyfybe, rwlryr0h, y3hgnb6u, ootj52fs, vdk2pxkk, 0b1qaecu, 9hru9fyg, vo2oe2wr, piwdsazx, uhrijlyz, mj9ea464, i6vuhaiv, fu0x6bqb, qg2pb884, ret6iq3o, 7isxu2jm, fvrhzc7x, pq0atbfg, x4fbph3d, 8ext7xa7, dksh41wz, mvv1rg68, tmfrc2e2, x8uzlsn7, sb43y1cl, pbhdzshl, 7lzyvzg2, gu0930s3, qwh8ei60, 5yrouyvo, h1gnp62r, 5abus746, 8rs8ilio, 8kooncil, oir3rlb7, 5lmrthmb, wwf90zxt]\n",
      "\n",
      "Query tokens: ['retarded', 'white', 'woman']\n",
      "{5lmrthmb, wwf90zxt, zp9k1k3z}\n"
     ]
    }
   ],
   "source": [
    "query = 'retarded white woman'\n",
    "docs1, score = index.search(query, verbose=True)\n",
    "print(docs1)\n",
    "print()\n",
    "docs2 = index.advanced_search(query, verbose=True)\n",
    "print(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['retarded', '(', 'white', 'AND', 'woman', ')']\n"
     ]
    }
   ],
   "source": [
    "query = 'retarded    (white AND woman)'\n",
    "querytokens = re.split('([^a-zA-Z0-9])', query)\n",
    "# Gotta do this to capture parenthesis\n",
    "querytokens = chain.from_iterable([t.split() for t in querytokens])\n",
    "\n",
    "print(list(querytokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\\n"
     ]
    }
   ],
   "source": [
    "print('\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retarded True\n",
      "\n",
      "retarded OR white OR woman True\n",
      "\n",
      "retarded OR ( ( white NOT woman ) False\n",
      "['SyntaxError: Found stray opening parenthesis']\n",
      "\n",
      "AND retarded ) ) OR ( ( white NOT woman ) NOT False\n",
      "['SyntaxError: First token \"AND\" is an operator', 'SyntaxError: Last token \"NOT\" is an operator', 'SyntaxError: Found stray closing parenthesis']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation \n",
    "\n",
    "_PUNCTUATION = punctuation.replace('(','').replace(')','')\n",
    "\n",
    "def get_logical_querytokens(query: str):\n",
    "    query = re.sub(f'[{_PUNCTUATION}]','',query)\n",
    "    querytokens = re.split('([^a-zA-Z0-9])', query)\n",
    "    # Gotta do this to capture parenthesis\n",
    "    querytokens = chain.from_iterable([t.split() for t in querytokens])\n",
    "\n",
    "    q1 = deque(querytokens)\n",
    "    q2 = deque()\n",
    "\n",
    "    operators = {'OR','AND','NOT'}    \n",
    "    q2.append(q1.popleft())\n",
    "\n",
    "    p_counter = 0\n",
    "    while len(q1) > 0:\n",
    "        token = q1.popleft()\n",
    "        \n",
    "        if q2[-1] == '(' or token == ')':\n",
    "            q2.append(token)\n",
    "            continue\n",
    "        \n",
    "        # If preceeding token was operator\n",
    "        if q2[-1] in operators:\n",
    "            q2.append(token)\n",
    "        # If preceeding token was not operator\n",
    "        else:\n",
    "            # If current token is not an operator\n",
    "            if token not in operators:\n",
    "                q2.append('OR')\n",
    "            q2.append(token)\n",
    "    return q2\n",
    "\n",
    "def assert_query(querytokens: deque, errmsgs: list) -> bool:\n",
    "    '''\n",
    "    Check if query is properly formatted. Returns True if everything is ok,\n",
    "    else False. \n",
    "    '''\n",
    "    q = querytokens.copy()\n",
    "    operators = {'OR','AND','NOT'}\n",
    "    p_list = []\n",
    "    p_counter = 0 \n",
    "    flag = True\n",
    "    curr = q.popleft()\n",
    "    \n",
    "    ##### Initialize by checking the first token #####\n",
    "    \n",
    "    # Stray closing parenthesis\n",
    "    if curr == ')':\n",
    "        p_counter -= 1\n",
    "        p_list.append(p_counter)\n",
    "        flag = False\n",
    "    \n",
    "    if curr == '(':\n",
    "        p_counter += 1\n",
    "        p_list.append(p_counter)\n",
    "    \n",
    "    # If query is starting with an operator\n",
    "    if curr in operators:\n",
    "        errmsgs.append(f'SyntaxError: First token \"{curr}\" is an operator')\n",
    "        flag = False\n",
    "    \n",
    "    # If querytokens consisted of only a single token\n",
    "    if len(q) == 0:\n",
    "        if p_counter != 0:\n",
    "            errmsgs.append(f'SyntaxError: Stray parenthesis')\n",
    "            flag = False\n",
    "        return flag\n",
    "    ##################################################\n",
    "    \n",
    "    prev = curr\n",
    "    # Runs if more than one token left\n",
    "    while len(q) > 0:\n",
    "        curr: str = q.popleft()\n",
    "        \n",
    "        if curr == '(':\n",
    "            p_counter += 1\n",
    "            p_list.append(p_counter)\n",
    "        \n",
    "        if curr == ')':\n",
    "            p_counter -= 1\n",
    "            p_list.append(p_counter)\n",
    "        \n",
    "        # If curr is operator\n",
    "        if curr in operators:\n",
    "            # Two succeeding operators\n",
    "            if prev in operators:\n",
    "                errmsgs.append(f'SyntaxError: Two succeeding operators \"{prev} {curr}\"')\n",
    "                flag = False\n",
    "                \n",
    "        prev = curr\n",
    "    \n",
    "    # Should only be one token left when interpreter is here\n",
    "    \n",
    "    # If ending with an operator\n",
    "    if prev in operators:\n",
    "        errmsgs.append(f'SyntaxError: Last token \"{prev}\" is an operator')\n",
    "        flag = False\n",
    "    \n",
    "    ###### Check paranthesis' #####\n",
    "    \n",
    "    # If unbalanced number of parenthesis'\n",
    "    if p_counter > 0:\n",
    "        errmsgs.append(f'SyntaxError: Found stray opening parenthesis')\n",
    "        flag = False\n",
    "        \n",
    "    # Check if any negative values in p_list, implies stray closing brackets\n",
    "    if any((x < 0 for x in p_list)): \n",
    "        errmsgs.append(f'SyntaxError: Found stray closing parenthesis')\n",
    "        flag = False\n",
    "    ###############################\n",
    "    return flag\n",
    "\n",
    "queries = [\n",
    "    'retarded!!!',\n",
    "#     '(',\n",
    "#     ')',\n",
    "    'retarded white woman',\n",
    "#     'retarded OR white OR woman',\n",
    "#     'retarded white AND woman',\n",
    "#     'retarded OR white NOT woman',\n",
    "#     'retarded (white NOT woman)',\n",
    "#     'retarded (white NOT woman)',\n",
    "#     'OR retarded AND white woman',\n",
    "#     'retarded AND AND white NOT woman',\n",
    "#     'retarded (white NOT woman) AND',\n",
    "#     ')retarded ((white NOT woman) AND',\n",
    "    'retarded ((white NOT woman)',\n",
    "    'AND retarded)) ((white NOT woman) NOT',\n",
    "]\n",
    "    \n",
    "for query in queries:\n",
    "    errmsgs=[]\n",
    "    q = get_logical_querytokens(query)\n",
    "    print(str(' '.join(q)) +' '+ str(assert_query(q, errmsgs)))\n",
    "    if errmsgs: print(errmsgs)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque(['retarded', 'OR', '(', 'white', 'AND', 'woman', ')'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'retarded (white AND woman)'\n",
    "\n",
    "def parenthesis_handler(querytokens: deque):\n",
    "    pass\n",
    "    \n",
    "get_logical_querytokens(query)\n",
    "\n",
    "# ['retarded', 'OR', ['white', 'AND', 'woman']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cardinality 1: {len(docs1)}')\n",
    "print(f'Cardinality 2: {len(docs2)}')\n",
    "\n",
    "docs1_list = sorted(docs1, key=lambda x: x.uid)\n",
    "docs2_list = sorted(docs2, key=lambda x: x.uid)\n",
    "\n",
    "# Set equal gave unexpceted results\n",
    "print(f'Equal?: {all([d1 == d2 for d1, d2 in zip(docs1_list, docs2_list)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('textfile.txt', 'w+') as f:\n",
    "#     uid = docs2_list[0].uid\n",
    "#     uid = 'dlh93ax6'\n",
    "#     uid = 'zp9k1k3z'\n",
    "#     uid = 'vs5yondw'\n",
    "    uid = 'i94lyfsh'\n",
    "    \n",
    "    print(f'dumping {uid}')\n",
    "    f.write(df[df.cord_uid == uid].body_text.values[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, scores = index.search('retarded white woman', verbose=True)\n",
    "# docs, scores = index.search('!', verbose=True)\n",
    "n = 16\n",
    "print()\n",
    "for doc, score in zip(docs[:n], scores[:n]):\n",
    "    print(f'{doc.uid}   {str(doc.title)[:69]:<69}   {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import SentVectorDoc, Index, AI_Index\n",
    "\n",
    "def most_similar(token):\n",
    "    if token in model.wv:\n",
    "        return model.wv.most_similar(token)\n",
    "    return []\n",
    "\n",
    "def test_AI_Index(model):\n",
    "    coordle = AI_Index(most_similar, 3)\n",
    "    coordle.build_from_df(\n",
    "        df.iloc[:1024],\n",
    "        'cord_uid',\n",
    "        'title',\n",
    "        'body_text',\n",
    "        use_multiprocessing=True,\n",
    "        workers=-1,\n",
    "        verbose=True\n",
    "    )\n",
    "    return coordle\n",
    "\n",
    "ai_index = test_AI_Index(model)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, scores = ai_index.search('retarded white woman', verbose=True)\n",
    "n = 16\n",
    "print()\n",
    "for doc, score in zip(docs[:n], scores[:n]):\n",
    "    print(f'{doc.uid}   {doc.title[:80]:<80}   {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing showdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "with Pool(None) as p:\n",
    "    cleaned_texts = list(tqdm(p.imap(clean_text, df.body_text), position=0, total=len(df)))\n",
    "print(f'Text cleaning with multiprocessing took {time()-t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "gen = tqdm((clean_text(text) for text in df.body_text), position=0, total=len(df))\n",
    "cleaned_texts = list(gen)\n",
    "print(f'NaÃ¯ve text cleaning took {time()-t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "with Pool(None) as p:\n",
    "    cleaned_texts = list(tqdm(p.imap(clean_text, df.body_text[:2048]), position=0, total=2048))\n",
    "print(f'Text cleaning with multiprocessing took {time()-t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "gen = tqdm((clean_text(text) for text in df.body_text[:2048]), position=0, total=2048)\n",
    "cleaned_texts = list(gen)\n",
    "print(f'NaÃ¯ve text cleaning took {time()-t0:.2f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
