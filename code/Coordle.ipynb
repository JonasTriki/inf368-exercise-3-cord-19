{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Idea\n",
    "1. Get query, e.g. \"What is love?\"\n",
    "2. Tokenize query\n",
    "3. Create a vector using word2vec of the tokens by summing them, or finding the average or whatever.\n",
    "   lets call it the query vector. \n",
    "4. Find sentence vectors that are the closesest to the query vector.\n",
    "5. Return the papers corresponding to said sentence vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Frontend\n",
    "💩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import nltk\n",
    "import spacy\n",
    "import en_core_sci_lg # Biomedical word embeddings\n",
    "from utils import clean_text, load_pickle, save_pickle\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from collections.abc import Iterable\n",
    "from typing import Union\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re \n",
    "from string import punctuation as PUNCTUATION\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from os.path import join as join_path\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cord-19-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self, output_dir: str, prefix: str, logs_filename: str):\n",
    "        self.output_dir = output_dir\n",
    "        self.prefix = prefix\n",
    "        self.logs_filename = logs_filename\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        cum_loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            loss = cum_loss\n",
    "        else:\n",
    "            loss = cum_loss - self.loss_previous_step\n",
    "        self.loss_previous_step = loss\n",
    "        with open(join_path(self.output_dir, self.logs_filename), 'a+') as file:\n",
    "            file.write(f'Epoch #{self.epoch}, loss: {loss}\\n')\n",
    "        \n",
    "        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch}.model')\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1    \n",
    "\n",
    "class DocEpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self, output_dir: str, prefix: str, start_epoch: int = 1):\n",
    "        self.output_dir = output_dir\n",
    "        self.prefix = prefix\n",
    "        self.epoch = start_epoch\n",
    "\n",
    "    def on_epoch_end(self, model):        \n",
    "        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch}.model')\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last trained model\n",
    "model = Word2Vec.load(join_path('models-word2vec', 'w2v_model_epoch_29.model'))\n",
    "word_to_int = {word:i for i, word in enumerate(model.wv.index2word)}\n",
    "int_to_word = np.array(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning initilized on 16 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning texts: 100%|██████████| 256/256 [00:00<00:00, 503.94it/s]\n",
      "Adding to index: 100%|██████████| 256/256 [00:00<00:00, 401.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import SentVectorDoc, Index\n",
    "\n",
    "def test_SentVectorDoc():\n",
    "    docsample = df[df.cord_uid == 'vs5yondw']\n",
    "#     doc = SentVectorDoc(docsample['cord_uid'], docsample['title'])\n",
    "#     doc, _ = doc.fit(docsample['body_text'])\n",
    "    gaga = Index()\n",
    "    gaga.build_from_df(\n",
    "        docsample,\n",
    "#         df[:32],\n",
    "        'cord_uid',\n",
    "        'title',\n",
    "        'body_text', \n",
    "        verbose=True, \n",
    "        use_multiprocessing=False,\n",
    "        workers=-1\n",
    "    )\n",
    "    \n",
    "    return gaga\n",
    "#     return doc\n",
    "\n",
    "def test_Index():\n",
    "    coordle = Index()\n",
    "    coordle.build_from_df(\n",
    "        df.iloc[:256],\n",
    "        'cord_uid',\n",
    "        'title',\n",
    "        'body_text', \n",
    "        verbose=True, \n",
    "        use_multiprocessing=True,\n",
    "        workers=-1\n",
    "    )\n",
    "    return coordle\n",
    "\n",
    "index = test_Index()\n",
    "# fuck = test_SentVectorDoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle(index, 'coordle/coordle_index_8k_0904.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens:  ['retarded', 'white', 'woman']\n",
      "[8bnny6hj, zp9k1k3z, 5xswwney, fdtwagr1, hnl4a33n, 1rzbaj02, 15mv5yx7, 2ole6ya7, vs5yondw, j1iylpgm, ltf0xacf, eyk015n3, bgygebgx, mnhg329v, y2ruobm0, ak7bf0gq, uq88w4fm, i94lyfsh, lno01041, y2uhnlpd, ensep6lk, 9jpc42h1, tjzlssal, 34jn37sb, p8bzkmas, 67kyfybe, rwlryr0h, y3hgnb6u, ootj52fs, vdk2pxkk, 0b1qaecu, 9hru9fyg, vo2oe2wr, piwdsazx, uhrijlyz, mj9ea464, i6vuhaiv, fu0x6bqb, qg2pb884, ret6iq3o, 7isxu2jm, fvrhzc7x, pq0atbfg, x4fbph3d, 8ext7xa7, dksh41wz, mvv1rg68, tmfrc2e2, x8uzlsn7, sb43y1cl, pbhdzshl, 7lzyvzg2, gu0930s3, qwh8ei60, 5yrouyvo, h1gnp62r, 5abus746, 8rs8ilio, 8kooncil, oir3rlb7, 5lmrthmb, wwf90zxt]\n",
      "\n",
      "Query tokens: ['retarded', 'white', 'woman']\n",
      "deque(['retarded', 'white', 'woman'])\n",
      "{5lmrthmb, wwf90zxt, zp9k1k3z}\n"
     ]
    }
   ],
   "source": [
    "query = 'retarded white woman'\n",
    "docs1, score = index.search(query, verbose=True)\n",
    "print(docs1)\n",
    "print()\n",
    "docs2 = index.advanced_search(query, verbose=True)\n",
    "print(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['retarded', '(', 'white', 'AND', 'woman', ')']\n"
     ]
    }
   ],
   "source": [
    "query = 'retarded    (white AND woman)'\n",
    "querytokens = re.split('([^a-zA-Z0-9])', query)\n",
    "# Gotta do this to capture parenthesis\n",
    "querytokens = chain.from_iterable([t.split() for t in querytokens])\n",
    "\n",
    "print(list(querytokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([1, 10, 2, 3])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = deque([1,2,3])\n",
    "A.insert(1,10)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\\n"
     ]
    }
   ],
   "source": [
    "print('\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'(' in '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque(['retarded']) True\n",
      "\n",
      "deque(['retarded', 'OR', 'white', 'OR', 'woman']) True\n",
      "\n",
      "deque(['retarded', 'OR', 'white', 'OR', 'woman']) True\n",
      "\n",
      "deque(['retarded', 'OR', 'white', 'AND', 'woman']) True\n",
      "\n",
      "deque(['retarded', 'OR', 'white', 'NOT', 'woman']) True\n",
      "\n",
      "deque(['retarded', 'OR', '(', 'white', 'NOT', 'woman', ')']) True\n",
      "\n",
      "deque(['retarded', 'OR', '(', 'white', 'NOT', 'woman', ')']) True\n",
      "\n",
      "deque(['OR', 'retarded', 'AND', 'white', 'OR', 'woman']) False\n",
      "['SyntaxError: First token \"OR\" is an operator']\n",
      "\n",
      "deque(['retarded', 'AND', 'AND', 'white', 'NOT', 'woman']) False\n",
      "['SyntaxError: Two succeeding operators \"AND AND\"']\n",
      "\n",
      "deque(['retarded', 'OR', '(', 'white', 'NOT', 'woman', ')', 'AND']) False\n",
      "['SyntaxError: Last token \"AND\" is an operator']\n",
      "\n",
      "deque([')', 'OR', 'retarded', 'OR', '(', '(', 'white', 'NOT', 'woman', ')', 'AND']) False\n",
      "['SyntaxError: First token is stray closing parenthesis', 'SyntaxError: Last token \"AND\" is an operator']\n",
      "\n",
      "deque(['retarded', 'OR', '(', '(', 'white', 'NOT', 'woman', ')']) True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_logical_querytokens(query: str, errmsgs: list):\n",
    "    querytokens = re.split('([^a-zA-Z0-9])', query)\n",
    "    # Gotta do this to capture parenthesis\n",
    "    querytokens = chain.from_iterable([t.split() for t in querytokens])\n",
    "\n",
    "    q1 = deque(querytokens)\n",
    "    q2 = deque()\n",
    "\n",
    "    operators = {'OR','AND','NOT'}    \n",
    "    q2.append(q1.popleft())\n",
    "\n",
    "    p_counter = 0\n",
    "    while len(q1) > 0:\n",
    "        token = q1.popleft()\n",
    "        \n",
    "        if q2[-1] == '(':\n",
    "            q2.append(token)\n",
    "            p_counter += 1\n",
    "            continue\n",
    "            \n",
    "        if token == ')':\n",
    "            q2.append(token)\n",
    "            p_counter -= 1\n",
    "            continue\n",
    "        \n",
    "        # If preceeding token was operator\n",
    "        if q2[-1] in operators:\n",
    "            q2.append(token)\n",
    "        # If preceeding token was not operator\n",
    "        else:\n",
    "            # If current token is not an operator\n",
    "            if token not in operators:\n",
    "                q2.append('OR')\n",
    "            q2.append(token)\n",
    "    return q2\n",
    "\n",
    "def assert_query(querytokens: deque, errmsgs: list) -> bool:\n",
    "    '''\n",
    "    Check if query is properly formatted. Returns True is everthing is ok,\n",
    "    else False. \n",
    "    '''\n",
    "    q = querytokens.copy()\n",
    "    operators = {'OR','AND','NOT'}\n",
    "    flag = True\n",
    "    curr = q.popleft()\n",
    "    # If query is starting with an operator\n",
    "    if curr in operators:\n",
    "        errmsgs.append(f'SyntaxError: First token \"{curr}\" is an operator')\n",
    "        flag = False\n",
    "    \n",
    "    # Stray closing parenthesis\n",
    "    if curr == ')':\n",
    "        errmsgs.append(f'SyntaxError: First token is stray closing parenthesis')\n",
    "        flag = False\n",
    "    \n",
    "    # Handle case for 1 token\n",
    "    if len(q) == 0:\n",
    "        return flag\n",
    "    \n",
    "    prev = curr\n",
    "    # If more than one token left\n",
    "    while len(q) > 1:\n",
    "        curr = q.popleft()\n",
    "        # If curr is operator\n",
    "        if curr in operators:\n",
    "            # Two succeeding operators\n",
    "            if prev in operators:\n",
    "                errmsgs.append(f'SyntaxError: Two succeeding operators \"{prev} {curr}\"')\n",
    "                flag = False\n",
    "        prev = curr\n",
    "    \n",
    "    # Should only be one token left when interpreter is here\n",
    "    \n",
    "    # If ending with an operator\n",
    "    if q[0] in operators:\n",
    "        errmsgs.append(f'SyntaxError: Last token \"{q[0]}\" is an operator')\n",
    "        flag = False\n",
    "        \n",
    "    return flag\n",
    "\n",
    "queries = [\n",
    "    'retarded',\n",
    "    'retarded white woman',\n",
    "    'retarded OR white OR woman',\n",
    "    'retarded white AND woman',\n",
    "    'retarded OR white NOT woman',\n",
    "    'retarded (white NOT woman)',\n",
    "    'retarded (white NOT woman)',\n",
    "    'OR retarded AND white woman',\n",
    "    'retarded AND AND white NOT woman',\n",
    "    'retarded (white NOT woman) AND',\n",
    "    ')retarded ((white NOT woman) AND',\n",
    "    'retarded ((white NOT woman)',\n",
    "]\n",
    "    \n",
    "for query in queries:\n",
    "    errmsgs=[]\n",
    "    q = get_logical_querytokens(query, errmsgs)\n",
    "    print(str(q) +' '+ str(assert_query(q, errmsgs)))\n",
    "    if errmsgs: print(errmsgs)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinality 1: 62\n",
      "Cardinality 2: 3\n",
      "Equal?: False\n"
     ]
    }
   ],
   "source": [
    "print(f'Cardinality 1: {len(docs1)}')\n",
    "print(f'Cardinality 2: {len(docs2)}')\n",
    "\n",
    "docs1_list = sorted(docs1, key=lambda x: x.uid)\n",
    "docs2_list = sorted(docs2, key=lambda x: x.uid)\n",
    "\n",
    "# Set equal gave unexpceted results\n",
    "print(f'Equal?: {all([d1 == d2 for d1, d2 in zip(docs1_list, docs2_list)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping i94lyfsh\n"
     ]
    }
   ],
   "source": [
    "with open('textfile.txt', 'w+') as f:\n",
    "#     uid = docs2_list[0].uid\n",
    "#     uid = 'dlh93ax6'\n",
    "#     uid = 'zp9k1k3z'\n",
    "#     uid = 'vs5yondw'\n",
    "    uid = 'i94lyfsh'\n",
    "    \n",
    "    print(f'dumping {uid}')\n",
    "    f.write(df[df.cord_uid == uid].body_text.values[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens:  ['retarded', 'white', 'woman']\n",
      "\n",
      "8bnny6hj   Regional localization of virus in the central nervous system of mice    7.4594\n",
      "zp9k1k3z   Clinical and immunologic studies in identical twins discordant for sy   7.4227\n",
      "5xswwney   Interferon induction in swine lymphocyte antigen-defined miniature pi   5.0407\n",
      "fdtwagr1   Intracisternal virus-like particles in brain of a multiple sclerosis    4.5837\n",
      "hnl4a33n   Expression of Ia antigen on perivascular and microglial cells after s   4.1718\n",
      "1rzbaj02   Leptomeningitis and polycaryocyte formation in the CNS of rats inocul   4.0657\n",
      "15mv5yx7   Characterization of coronavirus JHM variants isolated from wistar fur   3.7061\n",
      "2ole6ya7   The pathogenesis of multiple sclerosis Additional considerations        3.2869\n",
      "vs5yondw   Acute bronchitis: Results of U.S. and European trials of antibiotic t   3.0751\n",
      "j1iylpgm   Characterization of murine hepatitis virus (JHM) RNA from rats with e   2.7213\n",
      "ltf0xacf   Paediatric gastroenteritis in the eastern Malaysian state of Sarawak:   2.4154\n",
      "eyk015n3   Electron-microscopic appearance of the DA virus, a demyelinating muri   2.3673\n",
      "bgygebgx   Feline non-suppurative meningoencephalomyelitis. A clinical and patho   2.3251\n",
      "mnhg329v   Folate malnutrition in tropical diarrhoeas                              2.2861\n",
      "y2ruobm0   Disposition of the car☐y-terminus tail of rabbit lactase-phlorizin hy   2.2852\n",
      "ak7bf0gq   Bovine coronavirus antigen in the host cell plasmalemma                 2.1295\n"
     ]
    }
   ],
   "source": [
    "docs, scores = index.search('retarded white woman', verbose=True)\n",
    "# docs, scores = index.search('!', verbose=True)\n",
    "n = 16\n",
    "print()\n",
    "for doc, score in zip(docs[:n], scores[:n]):\n",
    "    print(f'{doc.uid}   {str(doc.title)[:69]:<69}   {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import SentVectorDoc, Index, AI_Index\n",
    "\n",
    "def most_similar(token):\n",
    "    if token in model.wv:\n",
    "        return model.wv.most_similar(token)\n",
    "    return []\n",
    "\n",
    "def test_AI_Index(model):\n",
    "    coordle = AI_Index(most_similar, 3)\n",
    "    coordle.build_from_df(\n",
    "        df.iloc[:1024],\n",
    "        'cord_uid',\n",
    "        'title',\n",
    "        'body_text',\n",
    "        use_multiprocessing=True,\n",
    "        workers=-1,\n",
    "        verbose=True\n",
    "    )\n",
    "    return coordle\n",
    "\n",
    "ai_index = test_AI_Index(model)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, scores = ai_index.search('retarded white woman', verbose=True)\n",
    "n = 16\n",
    "print()\n",
    "for doc, score in zip(docs[:n], scores[:n]):\n",
    "    print(f'{doc.uid}   {doc.title[:80]:<80}   {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing showdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "with Pool(None) as p:\n",
    "    cleaned_texts = list(tqdm(p.imap(clean_text, df.body_text), position=0, total=len(df)))\n",
    "print(f'Text cleaning with multiprocessing took {time()-t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "gen = tqdm((clean_text(text) for text in df.body_text), position=0, total=len(df))\n",
    "cleaned_texts = list(gen)\n",
    "print(f'Naïve text cleaning took {time()-t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "with Pool(None) as p:\n",
    "    cleaned_texts = list(tqdm(p.imap(clean_text, df.body_text[:2048]), position=0, total=2048))\n",
    "print(f'Text cleaning with multiprocessing took {time()-t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "gen = tqdm((clean_text(text) for text in df.body_text[:2048]), position=0, total=2048)\n",
    "cleaned_texts = list(gen)\n",
    "print(f'Naïve text cleaning took {time()-t0:.2f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
