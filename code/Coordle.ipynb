{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Idea\n",
    "1. Get query, e.g. \"What is love?\"\n",
    "2. Tokenize query\n",
    "3. Create a vector using word2vec of the tokens by summing them, or finding the average or whatever.\n",
    "   lets call it the query vector. \n",
    "4. Find sentence vectors that are the closesest to the query vector.\n",
    "5. Return the papers corresponding to said sentence vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Frontend\n",
    "ðŸ’©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Backend\n",
    "Essentially need a good way to do kNN search. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import nltk\n",
    "import spacy\n",
    "import en_core_sci_lg # Biomedical word embeddings\n",
    "from utils import clean_text, load_pickle, save_pickle\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re \n",
    "from string import punctuation as PUNCTUATION\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from os.path import join as join_path\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cord-19-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cord_uid', 'paper_id', 'source', 'is_pmc', 'title', 'body_text', 'doi',\n",
       "       'pubmed_id', 'license', 'abstract', 'publish_time', 'authors',\n",
       "       'journal', 'url', 'language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self, output_dir: str, prefix: str, logs_filename: str):\n",
    "        self.output_dir = output_dir\n",
    "        self.prefix = prefix\n",
    "        self.logs_filename = logs_filename\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        cum_loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            loss = cum_loss\n",
    "        else:\n",
    "            loss = cum_loss - self.loss_previous_step\n",
    "        self.loss_previous_step = loss\n",
    "        with open(join_path(self.output_dir, self.logs_filename), 'a+') as file:\n",
    "            file.write(f'Epoch #{self.epoch}, loss: {loss}\\n')\n",
    "        \n",
    "        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch}.model')\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last trained model\n",
    "model = Word2Vec.load(join_path('models-word2vec', 'w2v_model_epoch_29.model'))\n",
    "word_to_int = {word:i for i, word in enumerate(model.wv.index2word)}\n",
    "int_to_word = np.array(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.trainables.syn1neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:01<00:00, 81.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import SentVectorDoc, Index\n",
    "\n",
    "docsample = df.iloc[0]\n",
    "def test_SentVectorDoc():\n",
    "    doc = SentVectorDoc(W, int_to_word, word_to_int, docsample['cord_uid'], \n",
    "                        store_sents=True)\n",
    "    doc, _ = doc.fit(docsample['body_text'])\n",
    "    print(doc.tf_idf_score)\n",
    "    return doc\n",
    "\n",
    "def test_Index():\n",
    "    coordle = Index(W, int_to_word, word_to_int)\n",
    "    for i in tqdm(range(128), position=0):\n",
    "        sample = df.iloc[i]\n",
    "        coordle.add(sample['cord_uid'], sample['title'], sample['body_text'])\n",
    "    return coordle\n",
    "\n",
    "index = test_Index()\n",
    "# fuck = test_SentVectorDoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens:  ['retarded', 'white', 'woman']\n",
      "zp9k1k3z 0.003937323529173992\n",
      "5xswwney 0.0025311665707896253\n",
      "fdtwagr1 0.0022391159142015546\n",
      "hnl4a33n 0.0020737272359471226\n",
      "1rzbaj02 0.0019657469998755617\n",
      "2ole6ya7 0.0015826874925502022\n",
      "vs5yondw 0.0014266594616439065\n",
      "eyk015n3 0.0012717443626019324\n",
      "bgygebgx 0.0011908829236034638\n",
      "ak7bf0gq 0.0011336476320710932\n",
      "y2ruobm0 0.0010934790151866844\n",
      "uq88w4fm 0.0010765838077918756\n",
      "mnhg329v 0.0010675742366997407\n",
      "i94lyfsh 0.0009631800515236531\n",
      "y2uhnlpd 0.000916057912493545\n",
      "ensep6lk 0.0007936163278726123\n",
      "9jpc42h1 0.0007421340543952382\n",
      "34jn37sb 0.0007126962553809504\n",
      "rwlryr0h 0.0006910479034395635\n",
      "vdk2pxkk 0.0006331542625928979\n",
      "ootj52fs 0.0006175226343580554\n",
      "67kyfybe 0.0005973089192080529\n",
      "9hru9fyg 0.0005376984816147478\n",
      "uhrijlyz 0.0004899197469188614\n",
      "qg2pb884 0.0004329878581733049\n",
      "pq0atbfg 0.0003680755426050817\n",
      "x8uzlsn7 0.0002594724825448824\n",
      "qwh8ei60 0.00021505510635494993\n",
      "pbhdzshl 0.0002081321408991797\n",
      "h1gnp62r 0.0001863078951119297\n",
      "8rs8ilio 0.00016480858617855978\n",
      "8kooncil 0.00015766333348891953\n",
      "oir3rlb7 0.0001270532369499291\n",
      "wwf90zxt 0\n",
      "5lmrthmb 0\n"
     ]
    }
   ],
   "source": [
    "results = index['retarded white woman']\n",
    "# [print(result._tf_idf_score) for result in results]\n",
    "for res, score in zip(*results):\n",
    "    print(res.uid, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
