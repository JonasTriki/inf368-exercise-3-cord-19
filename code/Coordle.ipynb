{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Idea\n",
    "1. Get query, e.g. \"What is love?\"\n",
    "2. Tokenize query\n",
    "3. Create a vector using word2vec of the tokens by summing them, or finding the average or whatever.\n",
    "   lets call it the query vector. \n",
    "4. Find sentence vectors that are the closesest to the query vector.\n",
    "5. Return the papers corresponding to said sentence vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Frontend\n",
    "ðŸ’©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import nltk\n",
    "import spacy\n",
    "import en_core_sci_lg # Biomedical word embeddings\n",
    "from utils import clean_text, load_pickle, save_pickle\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from collections.abc import Iterable\n",
    "from typing import Union\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re \n",
    "from string import punctuation as PUNCTUATION\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from os.path import join as join_path\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cord-19-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self, output_dir: str, prefix: str, logs_filename: str):\n",
    "        self.output_dir = output_dir\n",
    "        self.prefix = prefix\n",
    "        self.logs_filename = logs_filename\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        cum_loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            loss = cum_loss\n",
    "        else:\n",
    "            loss = cum_loss - self.loss_previous_step\n",
    "        self.loss_previous_step = loss\n",
    "        with open(join_path(self.output_dir, self.logs_filename), 'a+') as file:\n",
    "            file.write(f'Epoch #{self.epoch}, loss: {loss}\\n')\n",
    "        \n",
    "        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch}.model')\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1    \n",
    "\n",
    "class DocEpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self, output_dir: str, prefix: str, start_epoch: int = 1):\n",
    "        self.output_dir = output_dir\n",
    "        self.prefix = prefix\n",
    "        self.epoch = start_epoch\n",
    "\n",
    "    def on_epoch_end(self, model):        \n",
    "        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch}.model')\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last trained model\n",
    "model = Word2Vec.load(join_path('models-word2vec', 'w2v_model_epoch_29.model'))\n",
    "word_to_int = {word:i for i, word in enumerate(model.wv.index2word)}\n",
    "int_to_word = np.array(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning initilized on 16 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4196/4196 [00:12<00:00, 327.78it/s]\n",
      "Adding to index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4196/4196 [00:12<00:00, 335.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import CordDoc, Index2, RecursiveDescentParser\n",
    "Index = Index2\n",
    "\n",
    "def test_CordDoc():\n",
    "    docsample = df[df.cord_uid == 'vs5yondw']\n",
    "#     doc = CordDoc(docsample['cord_uid'], docsample['title'])\n",
    "#     doc, _ = doc.fit(docsample['body_text'])\n",
    "    gaga = Index()\n",
    "    gaga.build_from_df(\n",
    "        docsample,\n",
    "#         df[:32],\n",
    "        'cord_uid',\n",
    "        'title',\n",
    "        'body_text', \n",
    "        verbose=True, \n",
    "        use_multiprocessing=False,\n",
    "        workers=-1\n",
    "    )\n",
    "    \n",
    "    return gaga\n",
    "#     return doc\n",
    "\n",
    "def test_Index():\n",
    "    coordle = Index()\n",
    "    coordle.build_from_df(\n",
    "        df.iloc[:4196],\n",
    "        'cord_uid',\n",
    "        'title',\n",
    "        'body_text', \n",
    "        verbose=True, \n",
    "        use_multiprocessing=True,\n",
    "        workers=-1\n",
    "    )\n",
    "    return coordle\n",
    "\n",
    "index = test_Index()\n",
    "# fuck = test_CordDoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle(index, 'coordle/coordle_index_8k_0904.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ytzglkgk, nhrzc7tj, up7hlqck, xsjdy3yz, tloaa3v1, ggsstwcm, 3gnfbgeo, dt0jku7v, 5ulk3euw, 4r61gaw2, aza0pzud, ke1r2e1b, dbu93ufh, yhunwsdq, qav5okgk, wisaamn7, ragy7afs, sbixelq5, z5wjge0x, r8j6lhoc, ieex2xwx, 5is9kc52, qg2r691d, 4nhy35xn, d28li5ao, pad3x324, epylkxdx, cd8u42xo, l7kaj498, cxth3hyn, ab63ijxx, bcd1q5an, bnnl700a, cgks5ap7, lahy0jfx, zroskqmk, tyid60aa, u5qd9lcd, ipvrc5we, qkedjzqs, p4exyu56, n1kkcl1w, b18hnqsl, yci0a6bt, mg2zikyp, bb27z7h0, 8mvsqh6t, dkp0fwe3, 6eer4qgr, 4qfhltg4, a9wmor1i, niurdu7t, 3dpnmswf, uniz8tuc, f7ky61ds, gcjgfasj, dh9n3j23, pl6gd3w4, qatiqnac, eb3asgvi, hwjkbpqp, u6strrib, 82m3w06s, 3xeg6kue, 06e9lkwl, dbmfydyu, m81xn0fs, zwfxnd7r, ffiguxo8, mbv7pk21, twvd7yke, lluqlzlg, d5d8amk7, 0r1b44id, kzv9cmr1, shnejtjt, 7uukeqea, 0ht2ap9g, d2covv62, 6anybxnk, qww6pe61, 73mhcnng, 8jsjyeio, bf2i20nq, v08d3oag, sxohecoi, ikx68zp0, 5bg7tswp, ufjjengt, eva0soja, ow2xqhmp, nr8fwc8q, lzquegwc, l7rn00vq, l42klpz6, 6rujx4vs, zq22sfna, dp6vzgef, sqe7h05t, p1ajw8mq, xkqxawj2, 517gka4h, 3injqurc, ii50gly3, 9q64wnb8, 8plec8t4, rtn2tr7t, m93klw76, 7zl3bw17, mvpz1nv9, h2g4d0uk, m57b8g9n, 9ibswt32, imbxofkp, uzgfx0fc, 6l4s2gpb, z6kqt8n8, rafwlw0t, qx1k21cx, 4uua0zgp, 03bwk538, wo4xbyg8, y65lguub, y2ldq8ra, d9kkhzej, tlu7gtg3, sr53cekh, 6l0xl2cj, olcuq6tm, tolikanw, bp988sir, z4sutqos, g2hdsr4v, in6lifj5, qcbw8zkc, xn6n63fw, pwuzr2ta, ffk58hoo, zz7e17wp, qcvoxf4o, 69vuc6l9, fec4kt20, rj3nov53, 8zfxklef, 0p6x4lwx, 104sqoxz, ffwnl1n8, emdlh9d9, imgjl21b, j3i4zlgv, 4899cq99, ajnyt1km, 8em6kpwk, 3i0ymq7o, 50i6cxby, 37qoupvk, at4j87fn, uzzyws7p, z6ufx206, k7fyttiq, yu60lxmg, q2a5aogo, 66yurgph, ylm0utrd, ejha0oh6, 4psg0z3n, cbelhu32, zrz6jyma, fzdmfzv8, xf0pmog4, 8tovupdm, umvrwgaw, 5mtmvl7r, 75z35eyd, tfke4xfv, upy712aj, fhm8abxp, jnbq29c8, h5o2ksfk, qssbwz28, 6gw9cwra, a0kh4kap, zjooov81, es3hubn4, 2acwgm7u, 60ji2cnl, isw6jeir, kxze0470, ufw13pjx, s9mqzqu4, v9k7vpi8, opzfwh6b, etx3chnt, 4azsywuu, gmznmdgh, u7ior83b, q73goacw, 28uqefrg, 8gtumtxb, 4lrb73hx, acei0pn8, 47ema2dq, liqhbqxl, abgr0ft1, h180nsed, 5kqtxh6t, pwlcqavv, dy21jno8, kt9ja8nc, 77hizppz, atfe2avm, si9jhugj, jvs25q21, gowyfu55, ucmpbk76, 880nqc0f, qwe1hni0, gl9cjmno, qxkwjkif, rxn5lges, 5e4z3b4p, 8ckb20hi, oobydik2, vvx0t16l, z0w6dark, md0bcflu, au1nf394, 9voqa1oy, mupt0so6, a0bqliei, ku5v5cbm, glbqzr8o, xavtxl4m, gruzb1ne, pug11wk9, 4mhh29l0, evgw0as5, mh2hd5fe, 3keht8ge, 1j34m7kr, yvjrcgxu, qzm9wgde, rixuuytu, 6vpx4opa, 9khx93c0, sdumq61z, ipv8awq7, vuzbswvq, ldwb05xn, y8z8gkpb, mx4t6s6n, harzowk2, ruk4p8rm, mgvadkjv, q4tk96tq, rqjeacow, rnkawxrq, vkeyd28w, lnabmwgp, 1axxmq84, gjiwfual, yk9af4aj, k59gdemp, qcjbtflt, w8579f54, stxnr07r, pbhvz1k0, gsl84nv6, 6jyv0q5e, 85pu5mvm, hc1id8pj, qyflkmi9, hvtto0ei, pkcsqnp4, 00rk8fb5, v8wd4f3r, us29j281, 36v9f1yz, trpy9bbq, vsjy9oaf, u5q0jnpn, 6u6ff1d7, 87o0j4px, qm5a5c4b, wup6tig0, fxre60s9, 2brelywe, ln8ddyuj, 1pp7k1k6, c0v2iive, ee9tdq1z, 7cgqxt2m, 80acuak9, kj3ptk7x, b35h15hn, xenq90xj, kfkumsux, 1hps5owh, 91uj6sph, yx99kks6, esrguepi, 43pd2qsp, 19noki6p, okdiyk1m, qsw8ncdw, it4y1d1o, i8zikevn, 0jyzk5kt, 5b6z7cqx, yr06jwm1, igu984oc, yz0buegp, 0lyt8gfq, x7omipn2, kxtdv6q9, yy96yeu9, fl6wlhjs, ary5eafy, y0gdjn64, nu8q72l9, tkt8f19m, xq65k1bt, feh9iswj, jiy4cp4n, 9rjbmsvb, pjbr6yl2, wyhnquw6, ap0zwzzl, do5c85ad, t9cqzxtx, a9o2dvke, n6qipu17, p5ekryt6, ap5n6ijl, 5omt75qr, h3qhwka0, vhi6nszc, 3i7cawyd, pgz55drr, vuvgvz4n, utmt5sva, m71xkuo9, mynsxblo, f0vud3gu, dxhhf0th, mtcalbo5, qm7fjojv, 5gglmx9d, o4k7auph, 1o9tzwjg, wu2mogfa, gthanxy9, bhq8t5nq, gg4qi4tj, mq94yfs8, 4mad9xws, 8ctsa9sd, o32e9o0l, 5byak3tw, yawwtyxg, zccd1mq5, i7v9l6ii, v5glur2c, cxqzac4a, wp4rch6v, j4sicnep, 36bfeoqv, 7q5by7qw, ppuqvafd, 828inb8d, ww6uedar, dugsh7mp, 6ubcjqfo, mgb5dwt2, tihc3ldd, hor76pqs, tfcerilc, byd0iwhf, urm6lnzy, yujp9zpx, 7k59ogn0, qla8ee8m, 6sx9fbwf, y77c4ro4, oean0n3k, k11asjaw, bze6g6jx, a6b9z4lx, trl5qsyd, yepiag5m, isroae1l, ztkjm79p, saloatt2, 3l4di1k7, h4vd6yr6, dsjba5xq, kwjumabw, th0wddvc, 5xhgocmi, lzfd5zxq, b4l7b7vp, o1o6whfr, o6dd2vu1, 7ofqlw6w, uu53dfo6, m1tk6782, knswbb2d, zrmkq3mz, y2knt8g0, feohfmbo, mtmgur1u, 0vke9lch, gafcbmlh, rdpsxb4n, 35ujxvjo, 7hiss0j3, cuaiyw56, 2p7qrgx0, eflxldnr, 36nks5os, 0a5hvja1, vncjvjoa, q66dyx98, qdqnrhai, 4z7yfj5s, b0o96z4k, gr7ii84y, 3hxau5vt, 48wao8a0, 3jnkki5z, ywb9krdp, 6mc88dfg, vv7x6h1i, wczw4vn1, nm30wct0, t40fnk2h, ycs5rtoc, rqj6c17g, 9fr0m92p, fijy00cz, z7ohrrg8, 4vo9gpiu, 6qd5qyb0, suw7yov6, zlzig0nn, c6cjxq07, qc6941pm, 1yf4yism, qevosik3, kt58jhxz, pgqsezq2, f2zc5rjj, 1769ovyk, k9qiwy1b, yz578wou, n2e4s7bu, tyew8h5b, 3j2q83ll, ttu6xum8, 1fgz2fyr, z8cbbit3, 4p7gboaq, b6eiamup, br5qk5xb, 4owsb0bg, af5grv7l, 7xbcihz2, itr3b63z, 06boh550, x0bio5bl, 8y89p11a, rb098rnu, hpo8bboi, 69rf4ml0, gzxu7nkh, yqg4nz1p, aq2y9pm0, 97f2w57j, mhjmhlay, ia80cy8j, 7cz1fvdi, s3ayq5u4, mdky6rdx, jxqskt4k, 387htidy, jiw3cji1, 93yfs7ve, sophqqne, ovpgw054, kmdxxx76, iv18eiap, duqkpkll, 2w7hc6hp, ki9f6ssk, mxt7stat, vxg6b6d0, wurzy88k, 9ka3jdbf, h4matthe, vwnpred5, k9rih2xn, d0l0nqgh, cpc8i54y, k95wrory, lddmo1sv, 9yrvjlpx, nz06v56o, pjdogrs4, 9r62ffew, epyc37sz, 8tcw3cll, exnqnvgg, wa6k2kv6, c31xsyeo, jm60nxc2, skavefji, 2nko37oo, 3neheq2s, 3m3ytwwv, uylgyhs8, zul5kz62, 8m9ygzn5, iec4mvh7, btre0gjz, ayxfsx1q, nn09hh29, e076c93q, 7q2hbawl, 1r65yam5, 6f0q661e, zhry4h3a, kyw4ptr2, a3tammig, wd3ir3wg, xne08vbd, v09jj3a2, 976j65ji, k1r91nt5, 3535k3op, xb5x1jpu, f7lm2o14, innxn4qv, n9sih438, dg10rf2f, 6kkqoh7f, aj1ufof6, z7ygy25e, p8b3ini4, vlns6j0j, s1b080p3, gloxcpzq, ug5a9wx6, osioowtp, qabaolje, 3t97ubzc, ywlig1wl, nblmshni, uqr00nzd, 2w26l80a, bf1wmbyw, tjr6dvtt, 4gk5o1b8, fh0mhqhv, tvxpckxo, yxne28f0, rn4zitcs, i05h8csb, tbxar0tu, p7um7o87, md293k0m, 22s9umrj, 1023f83a, fxd533b4, g9cccwo1, d4ok8wxs, 4e8rx2sx, 6kpyuffh, 19sejitq, ubpbdqtg, daiikgth, qylm0koe, tmzzowky, zgywyubf, xjjnmhsn, qun1kyvw, 9kiekksx, f5ka7eru, 6lobyyj4, 1u2fkwx3, l8y70ewk, 4p398ewl, ebzlt1rb, o3hibw1h, oh7usp68, r3r2bc7v, iimjfpyn, q2y7fewk, zu2zujgz, trhkv18o, wzkdhktu, hsl3caj5, gf718lbl, b99y6n43, oy09rdrl, 8s4bsnlw, 4m1npwxo, y6ws3u6x, szzr24ji, penzacou, f07lb9kk, 6hret3xy, b8xb1f12, 24qey5i9, 279ur6zt, qdety098, 5eqdrd52, 1zzyfmkb, 96xoj10p, 34xdjzvv, gerhieav, wbj0w365, 00cf294x, jy364gv4, a8rkvaax, bn2loscc, svnn4ptw, h640qddv, 7vvj0vfs, 6olhl2pk, a9f6hh7t, 16032h3d, 74mthqxe, 176djnf5, ted64zo4, hs3wfffs, vnrmb3rh, 4ub3q2zu, nuioo1nh, 1mzq227n, cqc4fvbo, gb8tt1qe, bgt7n2pq, u1io62e3, ue2zl8yl, zmodjaju, bcwcjf01, gs5lgdm2, 9y0zr0we, 5s10b9zy, crdp813c, qcbskifq, 3slyskn7, pwps783y, 1xacvavs, cc7eh1mf, 6wu024ng, vnh9eb00, o32d3kaw, bggo5e9t, 3g3apbon, t2pgb4l9, 52vixim5, nvu5tqxb, pwtouv76, salby0fu, bnld9o72, vqi3db9w, jleccqqx, etpgwzrn, jzj8q25c, auwm2qj0, 264h6aao, 84zfabti, 6x3l453f, 6io780dm, xwfptmmv, zz7gjfqg, v8l5cj5h, 1dx2ox4n, e3wmpthk, 4yfohp9w, 0pt6dxb3, k154lmef, adfigzc1, 10ynhrl3, tx5ff5gk, 1ssh296a, 9463h041, lvyl76dt, 6xaots4h, ojyxs3cp, 57es391o, kmwncmq8, 3frxd1c1, aqjl4lj0, jitao9k0, 9uswwyxm, d3a06n3f, cse6bdz8, 41b8ar5w, j7fx64k1, 36f6amze, zkdneyx5, z3teb4al, 6f8vyziv, g5rmv9s8, b7wp0gqn, 5kapn32k, f0t5e46j, d0bk9gu5, jfnj6l41, ipw53jvv, f7f274lq, 49u2onq0, d08buwtu, 5kjhrmmb, fvx0xof9, s90geszi, bg4au9u2, 8b9qjk7k, 49it5pha, 96mrxt8a, nhxro0bj, chj07xz6, plcdwazu, qp3biiu8, b22cioi2, e4uyh2zb, bbv6f5gf, pw2xqhwa, rafvxgx1, ep7ezoko, tmawi4mk, nlubwytj, 8xvbnuv0, gdra8xhj, 7ik3iszp, tycbhz8a, delk2zeg, zuvy8htp, 2nxz69aq, gux1bh9m, grsv06j7, 5yjfc1rf, vvspjage, j4iwq2ld, qwhaesfk, i55gwn3s, ywve8c0u, y24kbzy1, wyy6yw2o, zptw1rkk, nn9gj0z1, 9g8rwsm1, nw7ymn3c, 97tcohzf, apho6ykl, oao75pzy, wvm2ua95, zg4mdru1, 0hlj6r10, otfnrarh, hmloggsp, shmephya, ri5v6u4x]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'retarded white woman'\n",
    "docs1, score, errmsgs = index.search(query, verbose=True)\n",
    "print(docs1)\n",
    "print()\n",
    "# docs2 = index.advanced_search(query, verbose=True)\n",
    "# print(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['retarded', '(', 'white', 'AND', 'woman', ')']\n"
     ]
    }
   ],
   "source": [
    "query = 'retarded    (white AND woman)'\n",
    "querytokens = re.split('([^a-zA-Z0-9])', query)\n",
    "# Gotta do this to capture parenthesis\n",
    "querytokens = chain.from_iterable([t.split() for t in querytokens])\n",
    "\n",
    "print(list(querytokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning initilized on 16 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [00:36<00:00, 437.32it/s]\n",
      "Adding to index:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13901/16000 [00:44<00:08, 251.18it/s]"
     ]
    }
   ],
   "source": [
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import CordDoc, Index2, RecursiveDescentParser, AI_Index2\n",
    "Index = Index2\n",
    "AI_Index = AI_Index2\n",
    "\n",
    "# ai_index = AI_Index(model.wv.most_similar, n_similars=1)\n",
    "ai_index = Index()\n",
    "ai_index.build_from_df(\n",
    "    df.iloc[:16000],\n",
    "    'cord_uid',\n",
    "    'title',\n",
    "    'body_text', \n",
    "    verbose=True, \n",
    "    use_multiprocessing=True,\n",
    "    workers=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import CordDoc, Index2, RecursiveDescentParser, AI_Index2\n",
    "Index = Index2\n",
    "AI_Index = AI_Index2\n",
    "\n",
    "docs, scores, errmsgs = ai_index.search('retarded AND woman')\n",
    "n = 69\n",
    "if errmsgs:\n",
    "    print(errmsgs)\n",
    "else:\n",
    "    for doc, score in zip(docs, scores):\n",
    "        print(f'{doc.uid}  {str(doc.title)[:70]:<70}  {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque(['retarded'])\n",
      "True\n",
      "\n",
      "deque(['retarded', 'OR', deque(['white', 'AND', deque(['woman', 'NOT', 'man'])])])\n",
      "True\n",
      "\n",
      "deque(['retarded', 'OR', deque(['white', 'AND', 'white']), 'OR', 'man'])\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import coordle_backend\n",
    "reload(coordle_backend)\n",
    "from coordle_backend import CordDoc, Index2, RecursiveDescentParser, AI_Index2\n",
    "Index = Index2\n",
    "\n",
    "queries = [\n",
    "    'retarded!!!',\n",
    "#     '(',\n",
    "#     ')',\n",
    "    'retarded (white AND (woman NOT man))',\n",
    "    'retarded (white AND white) man',\n",
    "#     'retarded OR white OR woman',\n",
    "#     'retarded white AND woman',\n",
    "#     'retarded OR white NOT woman',\n",
    "#     'retarded (white NOT woman)',\n",
    "#     'retarded (white NOT woman)',\n",
    "#     'OR retarded AND white woman',\n",
    "#     'retarded AND AND white NOT woman',\n",
    "#     'retarded (white NOT woman) AND',\n",
    "#     ')retarded ((white NOT woman) AND',\n",
    "#     'retarded ((white NOT woman)',\n",
    "#     'AND retarded)) ((white NOT woman) NOT',\n",
    "]\n",
    "\n",
    "rdp = RecursiveDescentParser(index.docmap)\n",
    "for query in queries:\n",
    "    errmsgs = []\n",
    "    tokens = rdp.get_logical_querytokens(query)\n",
    "    pass_ = rdp.assert_query(tokens, errmsgs)\n",
    "    \n",
    "#     print(tokens)\n",
    "    print(rdp.parenthesis_handler(tokens))\n",
    "#     print(errmsgs)\n",
    "    print(pass_)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping qzm9wgde\n"
     ]
    }
   ],
   "source": [
    "with open('textfile.txt', 'w+') as f:\n",
    "#     uid = docs2_list[0].uid\n",
    "#     uid = 'dlh93ax6'\n",
    "#     uid = 'zp9k1k3z'\n",
    "#     uid = 'vs5yondw'\n",
    "    uid = 'qzm9wgde'\n",
    "    \n",
    "    print(f'dumping {uid}')\n",
    "    f.write(df[df.cord_uid == uid].body_text.values[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing showdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "with Pool(None) as p:\n",
    "    cleaned_texts = list(tqdm(p.imap(clean_text, df.body_text), position=0, total=len(df)))\n",
    "print(f'Text cleaning with multiprocessing took {time()-t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "gen = tqdm((clean_text(text) for text in df.body_text), position=0, total=len(df))\n",
    "cleaned_texts = list(gen)\n",
    "print(f'NaÃ¯ve text cleaning took {time()-t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "with Pool(None) as p:\n",
    "    cleaned_texts = list(tqdm(p.imap(clean_text, df.body_text[:2048]), position=0, total=2048))\n",
    "print(f'Text cleaning with multiprocessing took {time()-t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "gen = tqdm((clean_text(text) for text in df.body_text[:2048]), position=0, total=2048)\n",
    "cleaned_texts = list(gen)\n",
    "print(f'NaÃ¯ve text cleaning took {time()-t0:.2f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
