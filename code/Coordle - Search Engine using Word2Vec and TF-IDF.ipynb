{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Install required pip packages. Please note that the internet must be switched on to install them in the Kaggle kernel.\n!pip install -U kneed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports\nimport os\nfrom os.path import join as join_path\nimport numpy as np\nrng_seed = 368\nnp.random.seed(rng_seed)\nimport pandas as pd\n\n# Silence NumbaPerformanceWarning for UMAP\nimport warnings\nfrom numba.errors import NumbaPerformanceWarning\nwarnings.filterwarnings(\"ignore\", category=NumbaPerformanceWarning)\nimport umap\nfrom sklearn.cluster import KMeans\n\nfrom kneed import KneeLocator\nfrom scipy.spatial.distance import pdist\nfrom gensim.models import Word2Vec\nfrom gensim.models.callbacks import CallbackAny2Vec\nimport plotly.express as px\n\nfrom IPython.display import IFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Coordle - Search Engine using Word2Vec and TF-IDF\nDue to the special circumstances of the COVID-19 pandemic, the students of the Selected Topics in Machine Learning (topic being \"Deep Learning\") course ([INF368 Spring 2020](https://www.uib.no/en/course/INF368?sem=2020v)) at the University of Bergen were asked to participate in the competition.\n\nIn this notebook, you will find a search engine for the articles in the CORD-19 dataset. We named it Coordle (from Google + CORD) and was made using TF-IDF and Word2Vec. The search engine can be found in the interactive cell below, or by clicking here: https://coordle.triki.no/."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%html\n<center><iframe src=\"https://coordle.triki.no/\" width=\"800\" height=\"600\" frameborder=\"0\" allowfullscreen/></center>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Table of contents\n1. Installing the Coordle library\n2. Data preprocessing\n3. Creating word embeddings from scratch using Gensim\n4. Visualize word embeddings using UMAP\n5. Creating a search engine using TF-IDF\n6. Task results\n7. Future work"},{"metadata":{},"cell_type":"markdown","source":"## 1. Installing the Coordle library\nWe have separated the code into two Github repositories. [The first one](https://github.com/JonasTriki/inf368-exercise-3-cord-19) is used for the data preprocessing and experimentation. [The second repository](https://github.com/JonasTriki/inf368-exercise-3-coordle) is where the Coordle library is maintained at and which we will use throughout the notebook. To run the cell below, please note that the internet must be switched on. This is to install the Coordle library."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n!pip install -U git+https://github.com/JonasTriki/inf368-exercise-3-coordle.git\n    \n# Import Coordle modules\nfrom coordle.preprocessing import CORD19Data\nfrom coordle.utils import clean_text\nfrom coordle.backend import QueryAppenderIndex","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data preprocessing\nTo load and preprocess the CORD-19 data, we take inspiration from [Daniel Wolffram's \"CORD-19: Create Dataframe\" Notebook](https://www.kaggle.com/danielwolffram/cord-19-create-dataframe) and the [\"Date updates thread\"](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/137474) from the challenge itself. The goal of the data preprocessing is to get a single .csv file with all the cleaned/parsed data in place.\n\nIn particular, we first load the `metadata.csv` file using Pandas and perform some cleaning on it (dropping duplicates and articles without metadata). Then, we go through each and every row of the .csv file and parse it. We ensure that each row has either a PDF or PMC parse, and we prefer the PMC over PDF articles. For the body text of each article, we remove cite spans from the text since they are useless for creating word embeddings. We observed there were some false positive articles that had less than around 1000 characters in the body text and we exclude these.\n\nNext, we remove duplicate articles that have the same abstract/body_text and we extract the language from the article using spaCy. We do this because we only would like to have english articles in our final dataframe. After this, we save the result using Pandas. For more details about how we preprocessed the data, please consult the [cord_19_data.py](https://github.com/JonasTriki/inf368-exercise-3-coordle/blob/master/coordle/preprocessing/cord_19_data.py) file from the Coordle library repository."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define some constants\nkaggle_input_dir = join_path('/', 'kaggle', 'input')\ncord_data_raw_dir = join_path(kaggle_input_dir, 'CORD-19-research-challenge')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform preprocessing on the raw data\ncord_df = CORD19Data(cord_data_raw_dir).process_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Sanity check the processed dataframe\ncord_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Creating word embeddings from scratch using Gensim\nTo create the word embeddings for the CORD-19 dataset, we used Gensim and the `Word2Vec` class. However, before we can train the model we first define some helper classes. In particular, we define a data interator that yields sentences for Word2Vec to train on and a callback that saves an intermediate model after each epoch. The data iterator uses the `clean_text` function from the Coordle library. In short, it cleans the text by turning it into lowercase, removing punctuations, stopwords, numerics and words with one character. At last, it lemmatizes the text (turning the word `viruses` into `virus` for instance). The code for the function can be found [by clicking here](https://github.com/JonasTriki/inf368-exercise-3-coordle/blob/master/coordle/utils/utils.py#L40)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implement the data interator for Word2Vec\nclass CORDDataIteratorWord2Vec():\n    def __init__(self, texts: np.ndarray):\n        self.texts = texts\n    \n    def __iter__(self):\n        for text in self.texts:\n            sentences = nltk.tokenize.sent_tokenize(text)\n            cleaned_sentences = [clean_text(sent) for sent in sentences]\n            for sentence in cleaned_sentences:\n                yield sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implement the epoch saver for Word2Vec\nclass EpochSaver(CallbackAny2Vec):\n    '''Callback to save model after each epoch.'''\n\n    def __init__(self, output_dir: str, prefix: str, start_epoch: int = 1):\n        self.output_dir = output_dir\n        self.prefix = prefix\n        self.epoch = start_epoch\n\n    def on_epoch_end(self, model):\n        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch}.model')\n        model.save(output_path)\n        self.epoch += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we have defined these two classes, we train the model in three steps:\n1. Initialize Word2Vec model\n2. Build Word2Vec vocabulary\n3. Train the model\n\nWe split the steps into three parts to further sanity check that we did not make any mistakes on the way.\n\nThis is illustrated in the code below and takes around ~10 hours to run. For your converience, we have imported the final model/weights into the kernel in the `input/gensim-word2vec-model` folder after running for 20 epochs.\n```python\n# Extract English only texts\ncord_df_eng = cord_df[cord_df['language'] == 'en']\neng_texts = cord_df_eng['body_text'].values\n\ncord_sentences = CORDDataIteratorWord2Vec(eng_texts)\nw2v_saved_models_dir = 'models-word2vec'\nsaved_models_prefix = 'model'\n\n# 1. Setup initial model\nw2v_model = Word2Vec(\n    min_count=20,\n    window=2,\n    size=300,\n    negative=5,\n    callbacks=[EpochSaver(w2v_saved_models_dir, saved_models_prefix)]\n)\n\n# 2. Build vocabulary\nw2v_model.build_vocab(tqdm(cord_sentences, total=cord_num_sentences), progress_per=int(cord_num_sentences / 100))\n\n# 3. Train model\nw2v_model.train(\n    cord_sentences,\n    total_examples=w2v_model.corpus_count,\n    epochs=20,\n    report_delay=30\n)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the trained Gensim model\nmodel_path = join_path(kaggle_input_dir, 'gensim-word2vec-model', 'cord-19-w2v.model')\nw2v_model = Word2Vec.load(model_path)\nword_embedding_matrix = w2v_model.trainables.syn1neg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test word embeddings by finding most similar word vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar('covid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar('virus')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar('pandemic')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe above that the word embeddings do indeed make sense, when exploring a few examples. Next we will visualize the embeddings as well to get a deeper understanding of it."},{"metadata":{},"cell_type":"markdown","source":"## 4. Visualize word embeddings using K-means clustering and UMAP\nWe decided to use K-means clustering and UMAP to cluster and reduce the dimentionality of the word embeddings. This part is mainly as a sanity check to see that the word embeddings we have gotten from the Word2Vec algorithm actually make sense. To find the best number of clusters for K-means, we use the elbow method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cluster\nmin_k = 2\nks = np.arange(min_k, 21)\nerrors = np.zeros(len(ks))\nclusterings = np.zeros((len(ks), word_embedding_matrix.shape[0]))\nfor k in ks:\n    print(f'Clustering using k={k}...')\n    clusterer = KMeans(n_clusters=k, n_jobs=-1)\n    pred_labels = clusterer.fit_predict(word_embedding_matrix)\n    clusterings[k - min_k] = pred_labels\n    errors[k - min_k] = clusterer.inertia_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the elbow plot to determine the best k\nkneedle = KneeLocator(ks, errors, S=1.0, curve='convex', direction='decreasing')\nkneedle.plot_knee()\n\n# Select best clustering\nbest_clustering = clusterings[kneedle.knee - min_k]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce dimensionality using UMAP (with default params)\nword_embedding_3d = umap.UMAP(n_components=3).fit_transform(word_embedding_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the words in 3D with Plotly\nword_embedding_vis_df = pd.DataFrame({\n    'x': word_embedding_3d[:, 0],\n    'y': word_embedding_3d[:, 1],\n    'z': word_embedding_3d[:, 2],\n    'cluster_label': best_clustering,\n    'word': w2v_model.wv.index2word\n})\nfig = px.scatter_3d(word_embedding_vis_df, x='x', y='y', z='z', color='cluster_label', hover_name='word')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By zooming into some of the smaller clusters, we observe that months such as feburary and august are clustered together. We also observe that we get some clusters with date related words and words that represent temperatures, as well as some outliers here and there, which is not too unexpected."},{"metadata":{},"cell_type":"markdown","source":"## 5. Creating a search engine using TF-IDF\n- Intro\n- How we did it\n- Explain set-operators AND, OR, NOT\n- Combining with word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To demonstrate how the search engine works, we index on a subset of the documents in the CORD-19 dataframe.\nai_index = QueryAppenderIndex(w2v_model.wv.most_similar, n_similars=1)\nai_index.build_from_df(\n    cord_df[:1000],\n    'cord_uid',\n    'title',\n    'body_text', \n    verbose=True, \n    use_multiprocessing=True,\n    workers=-1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search_and_show(query: str, max_results: int = 5, max_body_length: int = 500):\n    '''Searches using the AI Index and shows the result\n    \n    Args:\n        query: Search query\n        max_results: Max results to show for each query    \n    '''\n    docs, scores, errmsgs = ai_index.search(query)\n    if errmsgs:\n        print('The following errors occurred:', errmsgs)\n    else:\n        if len(docs) == 0:\n            print('Sorry, no results found.')\n        else:\n            for doc, score in zip(docs[:max_results], scores[:max_results]):\n                print(f'{doc.uid}  {str(doc.title)[:70]:<70}  {score:.4f}')\n                print('---')\n                print(f'{cord_df[cord_df.cord_uid == doc.uid].body_text.values[0][:max_body_length]} {...}')\n                print('---')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_and_show('virus')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_and_show('virus AND')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_and_show('coronavirus symptoms in humans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Task results\nDue to computational limitations, we only indexed a few of the 35k+ articles in our dataset. To show the task results, we simply performed the following queries on the live website which has all articles indexed. For simplicity reasons, we only show the top 3 results for each query.\n1. What do we know about COVID-19 risk factors?\n    - covid AND risk AND factors\n2. What do we know about vaccines and therapeutics?\n    - covid AND vaccines\n    - covid AND therapy\n3. What has been published about medical care?\n    - covid AND medical care\n4. What do we know about diagnostics and surveillance?\n    - covid AND diagnostics\n    - covid AND surveillance"},{"metadata":{},"cell_type":"markdown","source":"### 6.1. What do we know about COVID-19 risk factors?\n![](https://i.imgur.com/j56hQIT.png)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 6.2. What do we know about vaccines and therapeutics?\n![](https://i.imgur.com/nSzQdDT.png)\n![](https://i.imgur.com/w1YQ8Yd.png)"},{"metadata":{},"cell_type":"markdown","source":"### 6.3. What has been published about medical care?\n![](https://i.imgur.com/upnLRTU.png)"},{"metadata":{},"cell_type":"markdown","source":"### 6.4. What do we know about diagnostics and surveillance?\n![](https://i.imgur.com/V8jrFc9.png)\n![](https://i.imgur.com/vai3KW0.png)"},{"metadata":{},"cell_type":"markdown","source":"## 7. Future work\n- Database indexing instead of in-memory\n- Highlighting of search query words\n- More (/better?) suggestions using Doc2Vec\n- cat AND dog AI Index"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}