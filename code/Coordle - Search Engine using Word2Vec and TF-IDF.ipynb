{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required pip packages. Please note that the internet must be switched on to install them in the Kaggle kernel.\n",
    "!pip install -U kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from os.path import join as join_path\n",
    "import numpy as np\n",
    "rng_seed = 368\n",
    "np.random.seed(rng_seed)\n",
    "import pandas as pd\n",
    "\n",
    "# Silence NumbaPerformanceWarning for UMAP\n",
    "import warnings\n",
    "from numba.errors import NumbaPerformanceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=NumbaPerformanceWarning)\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from kneed import KneeLocator\n",
    "from scipy.spatial.distance import pdist\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import plotly.express as px\n",
    "\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordle - Search Engine using Word2Vec and TF-IDF\n",
    "Due to the special circumstances of the COVID-19 pandemic, the students of the Selected Topics in Machine Learning (topic being \"Deep Learning\") course ([INF368 Spring 2020](https://www.uib.no/en/course/INF368?sem=2020v)) at the University of Bergen were asked to participate in the competition.\n",
    "\n",
    "In this notebook, you will find a search engine for the articles in the CORD-19 dataset. We named it Coordle (from Google + CORD) and was made using TF-IDF and Word2Vec. The search engine can be found in the interactive cell below, or by clicking here: https://coordle.triki.no/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe src=\"https://coordle.triki.no/\" width=\"800\" height=\"600\" frameborder=\"0\" allowfullscreen/></center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><iframe src=\"https://coordle.triki.no/\" width=\"800\" height=\"600\" frameborder=\"0\" allowfullscreen/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "1. [Installing the Coordle library](#installing_cord_library)\n",
    "2. [Data preprocessing](#data_preprocessing)\n",
    "3. [Creating word embeddings from scratch using Gensim](#create_word2vec)\n",
    "4. [Visualize word embeddings using UMAP](#visualize_word_embeddings)\n",
    "5. [Creating a search engine using TF-IDF](#search_engine_tf_idf)\n",
    "6. [Task results](#task_results)\n",
    "7. [Future work](#future_work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='installing_cord_library'></a>\n",
    "## 1. Installing the Coordle library\n",
    "We have separated the code into two Github repositories. [The first one](https://github.com/JonasTriki/inf368-exercise-3-cord-19) is used for the data preprocessing and experimentation. [The second repository](https://github.com/JonasTriki/inf368-exercise-3-coordle) is where the Coordle library is maintained at and which we will use throughout the notebook. To run the cell below, please note that the internet must be switched on. This is to install the Coordle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
    "!pip install -U git+https://github.com/JonasTriki/inf368-exercise-3-coordle.git\n",
    "    \n",
    "# Import Coordle modules\n",
    "from coordle.preprocessing import CORD19Data\n",
    "from coordle.utils import clean_text\n",
    "from coordle.backend import QueryAppenderIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_preprocessing'></a>\n",
    "## 2. Data preprocessing\n",
    "To load and preprocess the CORD-19 data, we take inspiration from [Daniel Wolffram's \"CORD-19: Create Dataframe\" Notebook](https://www.kaggle.com/danielwolffram/cord-19-create-dataframe) and the [\"Date updates thread\"](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/137474) from the challenge itself. The goal of the data preprocessing is to get a single .csv file with all the cleaned/parsed data in place.\n",
    "\n",
    "In particular, we first load the `metadata.csv` file using Pandas and perform some cleaning on it (dropping duplicates and articles without metadata). Then, we go through each and every row of the .csv file and parse it. We ensure that each row has either a PDF or PMC parse, and we prefer the PMC over PDF articles. For the body text of each article, we remove cite spans from the text since they are useless for creating word embeddings. We observed there were some false positive articles that had less than around 1000 characters in the body text and we exclude these.\n",
    "\n",
    "Next, we remove duplicate articles that have the same abstract/body_text and we extract the language from the article using spaCy. We do this because we only would like to have english articles in our final dataframe. After this, we save the result using Pandas. For more details about how we preprocessed the data, please consult the [cord_19_data.py](https://github.com/JonasTriki/inf368-exercise-3-coordle/blob/master/coordle/preprocessing/cord_19_data.py) file from the Coordle library repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "kaggle_input_dir = join_path('/', 'kaggle', 'input')\n",
    "cord_data_raw_dir = join_path(kaggle_input_dir, 'CORD-19-research-challenge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform preprocessing on the raw data\n",
    "cord_df = CORD19Data(cord_data_raw_dir).process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Sanity check the processed dataframe\n",
    "cord_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_word2vec'></a>\n",
    "## 3. Creating word embeddings from scratch using Gensim\n",
    "To create the word embeddings for the CORD-19 dataset, we used Gensim and the `Word2Vec` class. However, before we can train the model we first define some helper classes. In particular, we define a data interator that yields sentences for Word2Vec to train on and a callback that saves an intermediate model after each epoch. The data iterator uses the `clean_text` function from the Coordle library. In short, it cleans the text by turning it into lowercase, removing punctuations, stopwords, numerics and words with one character. At last, it lemmatizes the text (turning the word `viruses` into `virus` for instance). The code for the function can be found [by clicking here](https://github.com/JonasTriki/inf368-exercise-3-coordle/blob/master/coordle/utils/utils.py#L40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the data interator for Word2Vec\n",
    "class CORDDataIteratorWord2Vec():\n",
    "    def __init__(self, texts: np.ndarray):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for text in self.texts:\n",
    "            sentences = nltk.tokenize.sent_tokenize(text)\n",
    "            cleaned_sentences = [clean_text(sent) for sent in sentences]\n",
    "            for sentence in cleaned_sentences:\n",
    "                yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the epoch saver for Word2Vec\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self, output_dir: str, prefix: str, start_epoch: int = 1):\n",
    "        self.output_dir = output_dir\n",
    "        self.prefix = prefix\n",
    "        self.epoch = start_epoch\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        output_path = join_path(self.output_dir, f'{self.prefix}_epoch_{self.epoch}.model')\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have defined these two classes, we train the model in three steps:\n",
    "1. Initialize Word2Vec model\n",
    "2. Build Word2Vec vocabulary\n",
    "3. Train the model\n",
    "\n",
    "We split the steps into three parts to further sanity check that we did not make any mistakes on the way.\n",
    "\n",
    "This is illustrated in the code below and takes around ~10 hours to run. For your converience, we have imported the final model/weights into the kernel in the `input/gensim-word2vec-model` folder after running for 20 epochs.\n",
    "```python\n",
    "# Extract English only texts\n",
    "cord_df_eng = cord_df[cord_df['language'] == 'en']\n",
    "eng_texts = cord_df_eng['body_text'].values\n",
    "\n",
    "cord_sentences = CORDDataIteratorWord2Vec(eng_texts)\n",
    "w2v_saved_models_dir = 'models-word2vec'\n",
    "saved_models_prefix = 'model'\n",
    "\n",
    "# 1. Setup initial model\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=20,\n",
    "    window=2,\n",
    "    size=300,\n",
    "    negative=5,\n",
    "    callbacks=[EpochSaver(w2v_saved_models_dir, saved_models_prefix)]\n",
    ")\n",
    "\n",
    "# 2. Build vocabulary\n",
    "w2v_model.build_vocab(tqdm(cord_sentences, total=cord_num_sentences), progress_per=int(cord_num_sentences / 100))\n",
    "\n",
    "# 3. Train model\n",
    "w2v_model.train(\n",
    "    cord_sentences,\n",
    "    total_examples=w2v_model.corpus_count,\n",
    "    epochs=20,\n",
    "    report_delay=30\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Gensim model\n",
    "model_path = join_path(kaggle_input_dir, 'gensim-word2vec-model', 'cord-19-w2v.model')\n",
    "w2v_model = Word2Vec.load(model_path)\n",
    "word_embedding_matrix = w2v_model.trainables.syn1neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test word embeddings by finding most similar word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('virus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('pandemic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe above that the word embeddings do indeed make sense, when exploring a few examples. Next we will visualize the embeddings as well to get a deeper understanding of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualize_word_embeddings'></a>\n",
    "## 4. Visualize word embeddings using K-means clustering and UMAP\n",
    "We decided to use K-means clustering and UMAP to cluster and reduce the dimentionality of the word embeddings. This part is mainly as a sanity check to see that the word embeddings we have gotten from the Word2Vec algorithm actually make sense. To find the best number of clusters for K-means, we use the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster\n",
    "min_k = 2\n",
    "ks = np.arange(min_k, 21)\n",
    "errors = np.zeros(len(ks))\n",
    "clusterings = np.zeros((len(ks), word_embedding_matrix.shape[0]))\n",
    "for k in ks:\n",
    "    print(f'Clustering using k={k}...')\n",
    "    clusterer = KMeans(n_clusters=k, n_jobs=-1)\n",
    "    pred_labels = clusterer.fit_predict(word_embedding_matrix)\n",
    "    clusterings[k - min_k] = pred_labels\n",
    "    errors[k - min_k] = clusterer.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the elbow plot to determine the best k\n",
    "kneedle = KneeLocator(ks, errors, S=1.0, curve='convex', direction='decreasing')\n",
    "kneedle.plot_knee()\n",
    "\n",
    "# Select best clustering\n",
    "best_clustering = clusterings[kneedle.knee - min_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality using UMAP (with default params)\n",
    "word_embedding_3d = umap.UMAP(n_components=3).fit_transform(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the words in 3D with Plotly\n",
    "word_embedding_vis_df = pd.DataFrame({\n",
    "    'x': word_embedding_3d[:, 0],\n",
    "    'y': word_embedding_3d[:, 1],\n",
    "    'z': word_embedding_3d[:, 2],\n",
    "    'cluster_label': best_clustering,\n",
    "    'word': w2v_model.wv.index2word\n",
    "})\n",
    "fig = px.scatter_3d(word_embedding_vis_df, x='x', y='y', z='z', color='cluster_label', hover_name='word')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By zooming into some of the smaller clusters, we observe that months such as feburary and august are clustered together. We also observe that we get some clusters with date related words and words that represent temperatures, as well as some outliers here and there, which is not too unexpected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='search_engine_tf_idf'></a>\n",
    "## 5. Creating a search engine using TF-IDF\n",
    "\n",
    "#### Motivation and concept\n",
    "We want to obtain answers to the problems by creating our own search engine for the dataset. Naturally, we want the search engine to be fast and give relevant results when given a query. \n",
    "\n",
    "To achieve the desired searching speeds we pre-compute an index for the data. The index is a [hash table](https://en.wikipedia.org/wiki/Hash_table) that maps query tokens (e.g. words) to [sets](https://en.wikipedia.org/wiki/Set_(mathematics)) of documents that contains the tokens. \n",
    "\n",
    "To achieve relevant search results, we analyze the query and add tokens that are similar by utilizing word2vec. For example given the query \"fat cat\", we may effectively turn the query into \"fat obese cat dog\". The sets corresponding to each query can then for example be combined to a larger set. Then to score the relevance of the queried documents, we calculate the [TF-IDF](http://www.tfidf.com/) weights for each document. \n",
    "\n",
    "For example, when given the query \"covid symptoms\" the search engine essentially does:\n",
    "1.  Insert similar tokens to the query, e.g. corona and diagnose, effectively turning the query into \"covid corona symptoms diagnose\" \n",
    "2.  Obtain set $A$ of documents that contains \"covid\"\n",
    "3.  Obtain set $B$ of documents that contains \"corona\"\n",
    "4.  Obtain set $C$ of documents that contains \"symptoms\"\n",
    "5.  Obtain set $D$ of documents that contains \"diagnose\"\n",
    "6.  Obtain the union $E$ of the sets\n",
    "7.  Calculate the TF-IDF weights for each document in $E$ with respect to the query tokens\n",
    "8.  Return the documents sorted by the TF-IDF weights in descending order\n",
    "\n",
    "#### Query syntax with set operators\n",
    "Sometimes we would like to specify a more refined query. For example what if specifically we want the documents that contains the words \"corona\" and \"influenza\", without the word \"swine\". To enable this ability to the user, the search engine implements a parser that can parse set operators as well.\n",
    "\n",
    "<a id='syntax'></a>\n",
    "The search engine supports three operators: OR (union), AND (intersection), NOT (difference), from highest to lowest preceedence, meaning that the OR operator is evaluated before AND and so on. To override the order of preceedence, the user can use parenthesis', e.g. \"(cat AND dog) OR goose\" will be different from \"cat AND dog OR goose\". To sum it up with another example:\n",
    "\n",
    "Given the query \"cat AND virus\" the search engine essentially does:\n",
    "1.  Insert similar tokens in the query, e.g. dog and disease, effectively turning the query into \"(cat OR dog) AND (virus OR disease)\" \n",
    "2.  Obtain set $A$ set of documents that contains \"cat\"\n",
    "3.  Obtain set $B$ set of documents that contains \"dog\"\n",
    "4.  Obtain set $C$ set of documents that contains \"virus\"\n",
    "5.  Obtain set $D$ set of documents that contains \"disease\"\n",
    "6.  Let set $E$ be the result of evaluating $(A \\cup B) \\cap (C \\cup D)$\n",
    "7.  Calculate the TF-IDF weights for each document in $E$ with respect to the query tokens\n",
    "8.  Return the documents sorted by the TF-IDF weights in descending order\n",
    "\n",
    "Practically, the implementation of the search engine automatically adds OR operators between tokens that does not have any explicit operators, e.g. \"cat dog horse AND goose\" $\\Rightarrow$ \"cat OR dog OR horse AND goose \". To explicitly illustrate the effect of the preceedence, the query is equivalent to \"(cat OR dog OR horse) AND goose\".\n",
    "\n",
    "#### Indexing process\n",
    "We will now give a high level explanation on how the indexing process works. The source code for the indexing process is available [here](https://github.com/JonasTriki/inf368-exercise-3-coordle/blob/master/coordle/backend/coordle_backend.py#L537) if you so desire to examine the implementation details. \n",
    "\n",
    "Given a document $d_i$ from the collection of all documents $D$ that has an unique id $u_i$, and its corresponding text $t_i$ we do the following:\n",
    "1. Clean the text using the function ``clean_text`` mentioned in [section 3](#create_word2vec). The return value is a list containing text tokens. \n",
    "2. Get the unique tokens $\\tau_{ij}$ and their counts $c_{ij}$. ($i$ denotes document, $j$ denotes token index)\n",
    "3. For the document, create a dictionary $f_i$ that maps each token $\\tau_{ij}$ to their corresponding counts $c_{ij}$. \n",
    "\n",
    "We repeat this process for all the documents in $D$. \n",
    "\n",
    "#### Parser \n",
    "Without going into the details; the parsing method used to parse the [syntax](#syntax) is [recursive descent parsing](https://en.wikipedia.org/wiki/Recursive_descent_parser). The source code for the parser part of the source engine can be found [here](https://github.com/JonasTriki/inf368-exercise-3-coordle/blob/master/coordle/backend/coordle_backend.py#L421). \n",
    "\n",
    "<a id='parser_and_syntax_note'></a>\n",
    "##### Things to note about the parser and the syntax\n",
    "As a design choice, when using the NOT operator (set difference), the search engine will not append similar terms to the term that is used for the difference. For example: \"cat AND car\" may effectively become \"(cat OR dog) AND (car OR bus)\", while \"cat NOT car\" becomes \"(cat OR dog) NOT car\". This is to avoid removing potential relevant results. \n",
    "\n",
    "An issue that arises (that is yet to be handled) is that given the query \"cat NOT car sheep\" (all documents with cats but not car or sheep), the query effectively becomes for example \"(cat AND dog) NOT car OR (sheep OR cow)\", which effectively is \"(cat AND dog) NOT (car OR (sheep OR cow))\" because of operator preceedence. This may lead to a removal of a too large subset. For now the workaround the user has to do to ensure \"proper behaviour\" when using NOT is to chain them together; e.g \"cat NOT car NOT sheep\".\n",
    "\n",
    "#### TD-IDF \n",
    "[TF-IDF](http://tfidf.com/) stands for *term frequency-inverse document frequency*. It is a weight that is used often used for information retrieval and text mining. Given a collection of documents, TF-IDF is a statistical measure used to determine how strongly connected a word is to a specific document relatively to all the other documents. Given a word and a document from the collection, we compute TF (term frequency) and multiply it with IDF (inverse document frequency) to obtain its TF-IDF weight. The calculations are as follows:\n",
    "\n",
    "TF = (number of times the word appears in the document) / (total number of words in said document)\n",
    "\n",
    "IDF = log(documents in collection / number of documents with the word in it)\n",
    "\n",
    "The important parts to note for the query process are:\n",
    "- Given a query $Q$, the search engine must retrieve sets for each query token $q_i$.\n",
    "- Query $Q$ can have repeated tokens.\n",
    "- Right after retrieving the set corresponding to $q_i$, it calculates the TF-IDF score for each document in the set.\n",
    "- If a document is retrieved multiple times, it will accumulate TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To demonstrate how the search engine works, we index on a subset of the documents in the CORD-19 dataframe.\n",
    "ai_index = QueryAppenderIndex(w2v_model.wv.most_similar, n_similars=1)\n",
    "ai_index.build_from_df(\n",
    "    cord_df[:1000],\n",
    "    'cord_uid',\n",
    "    'title',\n",
    "    'body_text', \n",
    "    verbose=True, \n",
    "    use_multiprocessing=True,\n",
    "    workers=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_show(query: str, max_results: int = 3, max_body_length: int = 500):\n",
    "    '''Searches using the AI Index and shows the result\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        max_results: Max results to show for each query    \n",
    "    '''\n",
    "    docs, scores, errmsgs = ai_index.search(query)\n",
    "    if errmsgs:\n",
    "        print('The following errors occurred:', errmsgs)\n",
    "    else:\n",
    "        if len(docs) == 0:\n",
    "            print('Sorry, no results found.')\n",
    "        else:\n",
    "            for doc, score in zip(docs[:max_results], scores[:max_results]):\n",
    "                print(f'{doc.uid}  {str(doc.title)[:70]:<70}  {score:.4f}')\n",
    "                print('---')\n",
    "                print(f'{cord_df[cord_df.cord_uid == doc.uid].body_text.values[0][:max_body_length]} {...}')\n",
    "                print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_show('virus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_show('virus AND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_show('coronavirus symptoms in humans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task_results'></a>\n",
    "## 6. Task results\n",
    "Due to computational limitations, we only indexed a few of the 35k+ articles in our dataset. To show the task results, we simply performed the following queries on the live website which has all articles indexed. For simplicity reasons, we only show the top 3 results for each query.\n",
    "1. What do we know about COVID-19 risk factors?\n",
    "    - covid AND risk AND factors\n",
    "2. What do we know about vaccines and therapeutics?\n",
    "    - covid AND vaccines\n",
    "    - covid AND therapy\n",
    "3. What has been published about medical care?\n",
    "    - covid AND medical care\n",
    "4. What do we know about diagnostics and surveillance?\n",
    "    - covid AND diagnostics\n",
    "    - covid AND surveillance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. What do we know about COVID-19 risk factors?\n",
    "![](https://i.imgur.com/j56hQIT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. What do we know about vaccines and therapeutics?\n",
    "![](https://i.imgur.com/nSzQdDT.png)\n",
    "![](https://i.imgur.com/w1YQ8Yd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. What has been published about medical care?\n",
    "![](https://i.imgur.com/upnLRTU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. What do we know about diagnostics and surveillance?\n",
    "![](https://i.imgur.com/V8jrFc9.png)\n",
    "![](https://i.imgur.com/vai3KW0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='future_work'></a>\n",
    "## 7. Future work\n",
    "### Using database indexing instead of in-memory\n",
    "At the time of writing, we are currently indexing the articles into memory using our custom made TF-IDF search engine. The reason for doing this is mostly to enhance the search speed. We looked into indexing the database using [MongoDb](https://www.mongodb.com/), but after some experimentation we felt that the search time was infeasible for any production app (~24 seconds vs sub 1 second when indexed in memory). Now, by indexing all the articles into memory, the memory requirements are substantially higher than if we were to index the articles into a database/disk. We estimated around 16GB of memory required to store the whole index.\n",
    "\n",
    "### Highlighting search query words\n",
    "To further enhance the search experience for the user, we would like to emphasize or highlight the words from the search query in the web application. This way the user will get a better feeling of what search queries were used in the search, as they are currently kind of hidden in the Coordle backend (recall that the Coordle backend uses the word embeddings to add similar words to search query).\n",
    "\n",
    "### Enhance suggestions using Doc2Vec\n",
    "As of now we utilize word embeddings to widen the set of possible results by effectively appending similar tokens to given queries. In addition to this, we could for example find documents related to the top searches by using Doc2Vec and include them as well. \n",
    "\n",
    "### Alternative parser behavior\n",
    "An implication of the automatic addition of tokens in the query is that using set operators are technically not precise for the user. Since “cat AND car” effectively becomes for example “(cat OR dog) AND (car OR bus)”, one may be confused by the results, since one won’t necessarily get results that definitively contain cat and car. A solution for this is to maybe display what the effective query becomes so one always knows what it actually searches for, and also add the option to toggle whether you want the automatic addition of similar tokens. \n",
    "\n",
    "### Better handling of NOT operator\n",
    "As explained [here](#parser_and_syntax_note), the parser still needs some work when handling the NOT (difference) operator in the queries.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
